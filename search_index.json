[["index.html", "An Introduction to Statistical Learning Exercise solutions in R 1 Introduction", " An Introduction to Statistical Learning Exercise solutions in R 1 Introduction This bookdown document provides solutions for exercises in the book “An Introduction to Statistical Learning” by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. "],["statistical-learning.html", "2 Statistical Learning 2.1 Conceptual 2.2 Applied", " 2 Statistical Learning 2.1 Conceptual 2.1.1 Question 1 For each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer. The sample size n is extremely large, and the number of predictors p is small. Flexible best - opposite of b The number of predictors p is extremely large, and the number of observations n is small. Inflexible best - high chance of some predictors being randomly associated The relationship between the predictors and response is highly non-linear. Flexible best - inflexible leads to high bias The variance of the error terms, i.e. \\(\\sigma^2 = Var(\\epsilon)\\), is extremely high. Inflexible best - opposite of c 2.1.2 Question 2 Explain whether each scenario is a classification or regression problem, and indicate whether we are most interested in inference or prediction. Finally, provide n and p. We collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry and the CEO salary. We are interested in understanding which factors affect CEO salary. n=500, p=3, regression, inference We are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables. n=20, p=13, classification, prediction We are interested in predicting the % change in the USD/Euro exchange rate in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the USD/Euro, the % change in the US market, the % change in the British market, and the % change in the German market. n=52, p=3, regression, prediction 2.1.3 Question 3 We now revisit the bias-variance decomposition. Provide a sketch of typical (squared) bias, variance, training error, test error, and Bayes (or irreducible) error curves, on a single plot, as we go from less flexible statistical learning methods towards more flexible approaches. The x-axis should represent the amount of flexibility in the method, and the y-axis should represent the values for each curve. There should be five curves. Make sure to label each one. Explain why each of the five curves has the shape displayed in part (a). (squared) bias: Decreases with increasing flexibility (Generally, more flexible methods result in less bias). variance: Increases with increasing flexibility (In general, more flexible statistical methods have higher variance). training error: Decreases with model flexibility (More complex models will better fit the training data). test error: Decreases initially, then increases due to overfitting (less bias but more training error). Bayes (irreducible) error: fixed (does not change with model). 2.1.4 Question 4 You will now think of some real-life applications for statistical learning. Describe three real-life applications in which classification might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer. Coffee machine cleaned? (day of week, person assigned), inference. Is a flight delayed? (airline, airport etc). inference. Beer type (IPA, pilsner etc.), prediction. Describe three real-life applications in which regression might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer. Amount of bonus paid (profitability, client feedback), prediction. Person’s height, prediction. House price, inference. Describe three real-life applications in which cluster analysis might be useful. RNAseq tumour gene expression. SNPs in human populations. Types of client. 2.1.5 Question 5 What are the advantages and disadvantages of a very flexible (versus a less flexible) approach for regression or classification? Under what circumstances might a more flexible approach be preferred to a less flexible approach? When might a less flexible approach be preferred? Inflexible is more interpretable, fewer observations required, can be biased. Flexible can overfit (high error variance). In cases where we have high n or non-linear patterns flexible will be preferred. 2.1.6 Question 6 Describe the differences between a parametric and a non-parametric statistical learning approach. What are the advantages of a parametric approach to regression or classification (as opposed to a non-parametric approach)? What are its disadvantages? Parametric uses (model) parameters. Parametric models can be more interpretable as there is a model behind how data is generated. However, the disadvantage is that the model might not reflect reality. If the model is too far from the truth, estimates will be poor and more flexible models can fit many different forms and require more parameters (leading to overfitting). Non-parametric approaches do not estimate a small number of parameters, so a large number of observations may be needed to obtain accurate estimates. 2.1.7 Question 7 The table below provides a training data set containing six observations, three predictors, and one qualitative response variable. Obs. \\(X_1\\) \\(X_2\\) \\(X_3\\) \\(Y\\) 1 0 3 0 Red 2 2 0 0 Red 3 0 1 3 Red 4 0 1 2 Green 5 -1 0 1 Green 6 1 1 1 Red Suppose we wish to use this data set to make a prediction for \\(Y\\) when \\(X_1 = X_2 = X_3 = 0\\) using \\(K\\)-nearest neighbors. Compute the Euclidean distance between each observation and the test point, \\(X_1 = X_2 = X_3 = 0\\). dat &lt;- data.frame( &quot;x1&quot; = c(0, 2, 0, 0, -1, 1), &quot;x2&quot; = c(3, 0, 1, 1, 0, 1), &quot;x3&quot; = c(0, 0, 3, 2, 1, 1), &quot;y&quot; = c(&quot;Red&quot;, &quot;Red&quot;, &quot;Red&quot;, &quot;Green&quot;, &quot;Green&quot;, &quot;Red&quot;) ) # Euclidean distance between points and c(0, 0, 0) dist &lt;- sqrt(dat[[&quot;x1&quot;]]^2 + dat[[&quot;x2&quot;]]^2 + dat[[&quot;x3&quot;]]^2) signif(dist, 3) ## [1] 3.00 2.00 3.16 2.24 1.41 1.73 What is our prediction with \\(K = 1\\)? Why? knn &lt;- function(k) { names(which.max(table(dat[[&quot;y&quot;]][order(dist)[1:k]]))) } knn(1) ## [1] &quot;Green&quot; Green (based on data point 5 only) What is our prediction with \\(K = 3\\)? Why? knn(3) ## [1] &quot;Red&quot; Red (based on data points 2, 5, 6) If the Bayes decision boundary in this problem is highly non-linear, then would we expect the best value for \\(K\\) to be large or small? Why? Small (high k leads to linear boundaries due to averaging) 2.2 Applied 2.2.1 Question 8 This exercise relates to the College data set, which can be found in the file College.csv. It contains a number of variables for 777 different universities and colleges in the US. The variables are Private : Public/private indicator Apps : Number of applications received Accept : Number of applicants accepted Enroll : Number of new students enrolled Top10perc : New students from top 10% of high school class Top25perc : New students from top 25% of high school class F.Undergrad : Number of full-time undergraduates P.Undergrad : Number of part-time undergraduates Outstate : Out-of-state tuition Room.Board : Room and board costs Books : Estimated book costs Personal : Estimated personal spending PhD : Percent of faculty with Ph.D.’s Terminal : Percent of faculty with terminal degree S.F.Ratio : Student/faculty ratio perc.alumni : Percent of alumni who donate Expend : Instructional expenditure per student Grad.Rate : Graduation rate Before reading the data into R, it can be viewed in Excel or a text editor. Use the read.csv() function to read the data into R. Call the loaded data college. Make sure that you have the directory set to the correct location for the data. college &lt;- read.csv(&quot;data/College.csv&quot;) Look at the data using the View() function. You should notice that the first column is just the name of each university. We don’t really want R to treat this as data. However, it may be handy to have these names for later. Try the following commands: rownames(college) &lt;- college[, 1] View(college) You should see that there is now a row.names column with the name of each university recorded. This means that R has given each row a name corresponding to the appropriate university. R will not try to perform calculations on the row names. However, we still need to eliminate the first column in the data where the names are stored. Try college &lt;- college [, -1] View(college) Now you should see that the first data column is Private. Note that another column labeled row.names now appears before the Private column. However, this is not a data column but rather the name that R is giving to each row. rownames(college) &lt;- college[, 1] college &lt;- college[, -1] Use the summary() function to produce a numerical summary of the variables in the data set. Use the pairs() function to produce a scatterplot matrix of the first ten columns or variables of the data. Recall that you can reference the first ten columns of a matrix A using A[,1:10]. Use the plot() function to produce side-by-side boxplots of Outstate versus Private. Create a new qualitative variable, called Elite, by binning the Top10perc variable. We are going to divide universities into two groups based on whether or not the proportion of students coming from the top 10% of their high school classes exceeds 50%. &gt; Elite &lt;- rep(&quot;No&quot;, nrow(college)) &gt; Elite[college$Top10perc &gt; 50] &lt;- &quot;Yes&quot; &gt; Elite &lt;- as.factor(Elite) &gt; college &lt;- data.frame(college, Elite) Use the summary() function to see how many elite universities there are. Now use the plot() function to produce side-by-side boxplots of Outstate versus Elite. Use the hist() function to produce some histograms with differing numbers of bins for a few of the quantitative variables. You may find the command par(mfrow=c(2,2)) useful: it will divide the print window into four regions so that four plots can be made simultaneously. Modifying the arguments to this function will divide the screen in other ways. Continue exploring the data, and provide a brief summary of what you discover. summary(college) ## Private Apps Accept Enroll ## Length:777 Min. : 81 Min. : 72 Min. : 35 ## Class :character 1st Qu.: 776 1st Qu.: 604 1st Qu.: 242 ## Mode :character Median : 1558 Median : 1110 Median : 434 ## Mean : 3002 Mean : 2019 Mean : 780 ## 3rd Qu.: 3624 3rd Qu.: 2424 3rd Qu.: 902 ## Max. :48094 Max. :26330 Max. :6392 ## Top10perc Top25perc F.Undergrad P.Undergrad ## Min. : 1.00 Min. : 9.0 Min. : 139 Min. : 1.0 ## 1st Qu.:15.00 1st Qu.: 41.0 1st Qu.: 992 1st Qu.: 95.0 ## Median :23.00 Median : 54.0 Median : 1707 Median : 353.0 ## Mean :27.56 Mean : 55.8 Mean : 3700 Mean : 855.3 ## 3rd Qu.:35.00 3rd Qu.: 69.0 3rd Qu.: 4005 3rd Qu.: 967.0 ## Max. :96.00 Max. :100.0 Max. :31643 Max. :21836.0 ## Outstate Room.Board Books Personal ## Min. : 2340 Min. :1780 Min. : 96.0 Min. : 250 ## 1st Qu.: 7320 1st Qu.:3597 1st Qu.: 470.0 1st Qu.: 850 ## Median : 9990 Median :4200 Median : 500.0 Median :1200 ## Mean :10441 Mean :4358 Mean : 549.4 Mean :1341 ## 3rd Qu.:12925 3rd Qu.:5050 3rd Qu.: 600.0 3rd Qu.:1700 ## Max. :21700 Max. :8124 Max. :2340.0 Max. :6800 ## PhD Terminal S.F.Ratio perc.alumni ## Min. : 8.00 Min. : 24.0 Min. : 2.50 Min. : 0.00 ## 1st Qu.: 62.00 1st Qu.: 71.0 1st Qu.:11.50 1st Qu.:13.00 ## Median : 75.00 Median : 82.0 Median :13.60 Median :21.00 ## Mean : 72.66 Mean : 79.7 Mean :14.09 Mean :22.74 ## 3rd Qu.: 85.00 3rd Qu.: 92.0 3rd Qu.:16.50 3rd Qu.:31.00 ## Max. :103.00 Max. :100.0 Max. :39.80 Max. :64.00 ## Expend Grad.Rate ## Min. : 3186 Min. : 10.00 ## 1st Qu.: 6751 1st Qu.: 53.00 ## Median : 8377 Median : 65.00 ## Mean : 9660 Mean : 65.46 ## 3rd Qu.:10830 3rd Qu.: 78.00 ## Max. :56233 Max. :118.00 college$Private &lt;- college$Private == &quot;Yes&quot; pairs(college[, 1:10], cex = 0.2) plot(college$Outstate ~ factor(college$Private)) college$Elite &lt;- factor(ifelse(college$Top10perc &gt; 50, &quot;Yes&quot;, &quot;No&quot;)) summary(college$Elite) ## No Yes ## 699 78 plot(college$Outstate ~ college$Elite) par(mfrow = c(2,2)) hist(college$Enroll, breaks = 5, main = &quot;n = 5&quot;) hist(college$Enroll, breaks = 10, main = &quot;n = 10&quot;) hist(college$Enroll, breaks = 20, main = &quot;n = 20&quot;) hist(college$Enroll, breaks = 50, main = &quot;n = 50&quot;) chisq.test(college$Private, college$Elite) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: college$Private and college$Elite ## X-squared = 4.3498, df = 1, p-value = 0.03701 Whether a college is Private and Elite is not random! 2.2.2 Question 9 This exercise involves the Auto data set studied in the lab. Make sure that the missing values have been removed from the data. x &lt;- read.table(&quot;data/Auto.data&quot;, header = TRUE, na.strings = &quot;?&quot;) x &lt;- na.omit(x) Which of the predictors are quantitative, and which are qualitative? sapply(x, class) ## mpg cylinders displacement horsepower weight acceleration ## &quot;numeric&quot; &quot;integer&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; ## year origin name ## &quot;integer&quot; &quot;integer&quot; &quot;character&quot; numeric &lt;- which(sapply(x, class) == &quot;numeric&quot;) What is the range of each quantitative predictor? You can answer this using the range() function. sapply(x[, numeric], function(x) diff(range(x))) ## mpg displacement horsepower weight acceleration ## 37.6 387.0 184.0 3527.0 16.8 What is the mean and standard deviation of each quantitative predictor? sapply(x[, numeric], mean) ## mpg displacement horsepower weight acceleration ## 23.44592 194.41199 104.46939 2977.58418 15.54133 sapply(x[, numeric], sd) ## mpg displacement horsepower weight acceleration ## 7.805007 104.644004 38.491160 849.402560 2.758864 Now remove the 10th through 85th observations. What is the range, mean, and standard deviation of each predictor in the subset of the data that remains? sapply(x[-(10:85), numeric], function(x) diff(range(x))) ## mpg displacement horsepower weight acceleration ## 35.6 387.0 184.0 3348.0 16.3 sapply(x[-(10:85), numeric], mean) ## mpg displacement horsepower weight acceleration ## 24.40443 187.24051 100.72152 2935.97152 15.72690 sapply(x[-(10:85), numeric], sd) ## mpg displacement horsepower weight acceleration ## 7.867283 99.678367 35.708853 811.300208 2.693721 Using the full data set, investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment on your findings. pairs(x[, numeric], cex = 0.2) cor(x[, numeric]) ## mpg displacement horsepower weight acceleration ## mpg 1.0000000 -0.8051269 -0.7784268 -0.8322442 0.4233285 ## displacement -0.8051269 1.0000000 0.8972570 0.9329944 -0.5438005 ## horsepower -0.7784268 0.8972570 1.0000000 0.8645377 -0.6891955 ## weight -0.8322442 0.9329944 0.8645377 1.0000000 -0.4168392 ## acceleration 0.4233285 -0.5438005 -0.6891955 -0.4168392 1.0000000 heatmap(cor(x[, numeric])) Many of the variables appear to be highly (positively or negatively) correlated with some relationships being non-linear. Suppose that we wish to predict gas mileage (mpg) on the basis of the other variables. Do your plots suggest that any of the other variables might be useful in predicting mpg? Justify your answer. Yes, since other variables are correlated. However, horsepower, weight and displacement are highly related. 2.2.3 Question 10 This exercise involves the Boston housing data set. To begin, load in the Boston data set. The Boston data set is part of the ISLR2 library in R. &gt; library(ISLR2) Now the data set is contained in the object Boston. &gt; Boston Read about the data set: &gt; ?Boston How many rows are in this data set? How many columns? What do the rows and columns represent? library(ISLR2) dim(Boston) ## [1] 506 13 Make some pairwise scatterplots of the predictors (columns) in this data set. Describe your findings. library(ggplot2) library(tidyverse) ggplot(Boston, aes(nox, rm)) + geom_point() ggplot(Boston, aes(ptratio, rm)) + geom_point() heatmap(cor(Boston, method = &quot;spearman&quot;)) Are any of the predictors associated with per capita crime rate? If so, explain the relationship. Yes Do any of the census tracts of Boston appear to have particularly high crime rates? Tax rates? Pupil-teacher ratios? Comment on the range of each predictor. Boston |&gt; pivot_longer(cols = 1:13) |&gt; filter(name %in% c(&quot;crim&quot;, &quot;tax&quot;, &quot;ptratio&quot;)) |&gt; ggplot(aes(value)) + geom_histogram(bins = 20) + facet_wrap(~name, scales=&quot;free&quot;, ncol= 1) Yes, particularly crime and tax rates. How many of the census tracts in this data set bound the Charles river? sum(Boston$chas) ## [1] 35 What is the median pupil-teacher ratio among the towns in this data set? median(Boston$ptratio) ## [1] 19.05 Which census tract of Boston has lowest median value of owner-occupied homes? What are the values of the other predictors for that census tract, and how do those values compare to the overall ranges for those predictors? Comment on your findings. Boston[Boston$medv == min(Boston$medv), ] ## crim zn indus chas nox rm age dis rad tax ptratio lstat medv ## 399 38.3518 0 18.1 0 0.693 5.453 100 1.4896 24 666 20.2 30.59 5 ## 406 67.9208 0 18.1 0 0.693 5.683 100 1.4254 24 666 20.2 22.98 5 sapply(Boston, quantile) ## crim zn indus chas nox rm age dis rad tax ptratio ## 0% 0.006320 0.0 0.46 0 0.385 3.5610 2.900 1.129600 1 187 12.60 ## 25% 0.082045 0.0 5.19 0 0.449 5.8855 45.025 2.100175 4 279 17.40 ## 50% 0.256510 0.0 9.69 0 0.538 6.2085 77.500 3.207450 5 330 19.05 ## 75% 3.677083 12.5 18.10 0 0.624 6.6235 94.075 5.188425 24 666 20.20 ## 100% 88.976200 100.0 27.74 1 0.871 8.7800 100.000 12.126500 24 711 22.00 ## lstat medv ## 0% 1.730 5.000 ## 25% 6.950 17.025 ## 50% 11.360 21.200 ## 75% 16.955 25.000 ## 100% 37.970 50.000 In this data set, how many of the census tract average more than seven rooms per dwelling? More than eight rooms per dwelling? Comment on the census tracts that average more than eight rooms per dwelling. sum(Boston$rm &gt; 7) ## [1] 64 sum(Boston$rm &gt; 8) ## [1] 13 sapply(Boston[Boston$rm &gt; 8, ], median) ## crim zn indus chas nox rm age dis ## 0.52014 0.00000 6.20000 0.00000 0.50700 8.29700 78.30000 2.89440 ## rad tax ptratio lstat medv ## 7.00000 307.00000 17.40000 4.14000 48.30000 "],["linear-regression.html", "3 Linear Regression 3.1 Conceptual 3.2 Applied", " 3 Linear Regression 3.1 Conceptual 3.1.1 Question 1 Describe the null hypotheses to which the p-values given in Table 3.4 correspond. Explain what conclusions you can draw based on these p-values. Your explanation should be phrased in terms of sales, TV, radio, and newspaper, rather than in terms of the coefficients of the linear model. For intercept, that \\(\\beta_0 = 0\\) For the others, that \\(\\beta_n = 0\\) (for \\(n = 1, 2, 3\\)) We can conclude that that without any spending, there are still some sales (the intercept is not 0). Furthermore, we can conclude that money spent on TV and radio are significantly associated with increased sales, but the same cannot be said of newspaper spending. 3.1.2 Question 2 Carefully explain the differences between the KNN classifier and KNN regression methods. The KNN classifier is categorical and assigns a value based on the most frequent observed category among \\(K\\) nearest neighbors, whereas KNN regression assigns a continuous variable, the average of the response variables for the \\(K\\) nearest neighbors. 3.1.3 Question 3 Suppose we have a data set with five predictors, \\(X_1\\) = GPA, \\(X_2\\) = IQ, \\(X_3\\) = Level (1 for College and 0 for High School), \\(X_4\\) = Interaction between GPA and IQ, and \\(X_5\\) = Interaction between GPA and Level. The response is starting salary after graduation (in thousands of dollars). Suppose we use least squares to fit the model, and get \\(\\hat\\beta_0 = 50\\), \\(\\hat\\beta_1 = 20\\), \\(\\hat\\beta_2 = 0.07\\), \\(\\hat\\beta_3 = 35\\), \\(\\hat\\beta_4 = 0.01\\), \\(\\hat\\beta_5 = -10\\). Which answer is correct, and why? For a fixed value of IQ and GPA, high school graduates earn more on average than college graduates. For a fixed value of IQ and GPA, college graduates earn more on average than high school graduates. For a fixed value of IQ and GPA, high school graduates earn more on average than college graduates provided that the GPA is high enough. For a fixed value of IQ and GPA, college graduates earn more on average than high school graduates provided that the GPA is high enough. library(plotly) model &lt;- function(gpa, iq, level) { 50 + gpa * 20 + iq * 0.07 + level * 35 + gpa * iq * 0.01 + gpa * level * -10 } x &lt;- seq(1, 5, length = 10) y &lt;- seq(1, 200, length = 20) college &lt;- t(outer(x, y, model, level = 1)) high_school &lt;- t(outer(x, y, model, level = 0)) plot_ly(x = x, y = y) |&gt; add_surface( z = ~college, colorscale = list(c(0, 1), c(&quot;rgb(107,184,214)&quot;, &quot;rgb(0,90,124)&quot;)), colorbar = list(title = &quot;College&quot;)) |&gt; add_surface( z = ~high_school, colorscale = list(c(0, 1), c(&quot;rgb(255,112,184)&quot;, &quot;rgb(128,0,64)&quot;)), colorbar = list(title = &quot;High school&quot;)) |&gt; layout(scene = list( xaxis = list(title = &quot;GPA&quot;), yaxis = list(title = &quot;IQ&quot;), zaxis = list(title = &quot;Salary&quot;))) Option iii correct. Predict the salary of a college graduate with IQ of 110 and a GPA of 4.0. model(gpa = 4, iq = 110, level = 1) ## [1] 137.1 True or false: Since the coefficient for the GPA/IQ interaction term is very small, there is very little evidence of an interaction effect. Justify your answer. This is false. It is important to remember that GPA and IQ vary over different scales. It is better to explicitly test the significance of the interaction effect, and/or visualize or quantify the effect on sales under realistic ranges of GPA/IQ values. 3.1.4 Question 4 I collect a set of data (\\(n = 100\\) observations) containing a single predictor and a quantitative response. I then fit a linear regression model to the data, as well as a separate cubic regression, i.e. \\(Y = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\beta_3X^3 + \\epsilon\\). Suppose that the true relationship between \\(X\\) and \\(Y\\) is linear, i.e. \\(Y = \\beta_0 + \\beta_1X + \\epsilon\\). Consider the training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer. You would expect the cubic regression to have lower RSS since it is at least as flexible as the linear regression. Answer (a) using test rather than training RSS. Though we could not be certain, the test RSS would likely be higher due to overfitting. Suppose that the true relationship between \\(X\\) and \\(Y\\) is not linear, but we don’t know how far it is from linear. Consider the training RSS for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer. You would expect the cubic regression to have lower RSS since it is at least as flexible as the linear regression. Answer (c) using test rather than training RSS. There is not enough information to tell, it depends on how non-linear the true relationship is. 3.1.5 Question 5 Consider the fitted values that result from performing linear regression without an intercept. In this setting, the ith fitted value takes the form \\[\\hat{y}_i = x_i\\hat\\beta,\\] where \\[\\hat{\\beta} = \\left(\\sum_{i=1}^nx_iy_i\\right) / \\left(\\sum_{i&#39; = 1}^n x^2_{i&#39;}\\right).\\] show that we can write \\[\\hat{y}_i = \\sum_{i&#39; = 1}^na_{i&#39;}y_{i&#39;}\\] What is \\(a_{i&#39;}\\)? Note: We interpret this result by saying that the fitted values from linear regression are linear combinations of the response values. \\[\\begin{align} \\hat{y}_i &amp; = x_i \\frac{\\sum_{i=1}^nx_iy_i}{\\sum_{i&#39; = 1}^n x^2_{i&#39;}} \\\\ &amp; = x_i \\frac{\\sum_{i&#39;=1}^nx_{i&#39;}y_{i&#39;}}{\\sum_{i&#39;&#39; = 1}^n x^2_{i&#39;&#39;}} \\\\ &amp; = \\frac{\\sum_{i&#39;=1}^n x_i x_{i&#39;}y_{i&#39;}}{\\sum_{i&#39;&#39; = 1}^n x^2_{i&#39;&#39;}} \\\\ &amp; = \\sum_{i&#39;=1}^n \\frac{ x_i x_{i&#39;}y_{i&#39;}}{\\sum_{i&#39;&#39; = 1}^n x^2_{i&#39;&#39;}} \\\\ &amp; = \\sum_{i&#39;=1}^n \\frac{ x_i x_{i&#39;}}{\\sum_{i&#39;&#39; = 1}^n x^2_{i&#39;&#39;}} y_{i&#39;} \\end{align}\\] therefore, \\[a_{i&#39;} = \\frac{ x_i x_{i&#39;}}{\\sum x^2}\\] 3.1.6 Question 6 Using (3.4), argue that in the case of simple linear regression, the least squares line always passes through the point \\((\\bar{x}, \\bar{y})\\). when \\(x = \\bar{x}\\) what is \\(y\\)? \\[\\begin{align} y &amp;= \\hat\\beta_0 + \\hat\\beta_1\\bar{x} \\\\ &amp;= \\bar{y} - \\hat\\beta_1\\bar{x} + \\hat\\beta_1\\bar{x} \\\\ &amp;= \\bar{y} \\end{align}\\] 3.1.7 Question 7 It is claimed in the text that in the case of simple linear regression of \\(Y\\) onto \\(X\\), the \\(R^2\\) statistic (3.17) is equal to the square of the correlation between \\(X\\) and \\(Y\\) (3.18). Prove that this is the case. For simplicity, you may assume that \\(\\bar{x} = \\bar{y} = 0\\). We have the following equations: \\[ R^2 = \\frac{\\textit{TSS} - \\textit{RSS}}{\\textit{TSS}} \\] \\[ Cor(x,y) = \\frac{\\sum_i (x_i-\\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_i(x_i - \\bar{x})^2}\\sqrt{\\sum_i(y_i - \\bar{y})^2}} \\] As above, its important to remember \\(\\sum_i x_i = \\sum_j x_j\\) when \\(\\bar{x} = \\bar{y} = 0\\) \\[ Cor(x,y)^2 = \\frac{(\\sum_ix_iy_i)^2}{\\sum_ix_i^2 \\sum_iy_i^2} \\] Also note that: \\[\\hat{y}_i = \\hat\\beta_o + \\hat\\beta_1x_i = x_i\\frac{\\sum_j{x_jy_j}}{\\sum_jx_j^2}\\] Therefore, given that \\(RSS = \\sum_i(y_i - \\hat{y}_i)^2\\) and \\(\\textit{TSS} = \\sum_i(y_i - \\bar{y})^2 = \\sum_iy_i^2\\) \\[\\begin{align} R^2 &amp;= \\frac{\\sum_iy_i^2 - \\sum_i(y_i - x_i\\frac{\\sum_j{x_jy_j}}{\\sum_jx_j^2})^2} {\\sum_iy_i^2} \\\\ &amp;= \\frac{\\sum_iy_i^2 - \\sum_i( y_i^2 - 2y_ix_i\\frac{\\sum_j{x_jy_j}}{\\sum_jx_j^2} + x_i^2 (\\frac{\\sum_j{x_jy_j}}{\\sum_jx_j^2})^2 )}{\\sum_iy_i^2} \\\\ &amp;= \\frac{ 2\\sum_i(y_ix_i\\frac{\\sum_j{x_jy_j}}{\\sum_jx_j^2}) - \\sum_i(x_i^2 (\\frac{\\sum_j{x_jy_j}}{\\sum_jx_j^2})^2) }{\\sum_iy_i^2} \\\\ &amp;= \\frac{ 2\\sum_i(y_ix_i) \\frac{\\sum_j{x_jy_j}}{\\sum_jx_j^2} - \\sum_i(x_i^2) \\frac{(\\sum_j{x_jy_j})^2}{(\\sum_jx_j^2)^2} }{\\sum_iy_i^2} \\\\ &amp;= \\frac{ 2\\frac{(\\sum_i{x_iy_i})^2}{\\sum_jx_j^2} - \\frac{(\\sum_i{x_iy_i})^2}{\\sum_jx_j^2} }{\\sum_iy_i^2} \\\\ &amp;= \\frac{(\\sum_i{x_iy_i})^2}{\\sum_ix_i^2 \\sum_iy_i^2} \\end{align}\\] 3.2 Applied 3.2.1 Question 8 This question involves the use of simple linear regression on the Auto data set. Use the lm() function to perform a simple linear regression with mpg as the response and horsepower as the predictor. Use the summary() function to print the results. Comment on the output. For example: Is there a relationship between the predictor and the response? How strong is the relationship between the predictor and the response? Is the relationship between the predictor and the response positive or negative? What is the predicted mpg associated with a horsepower of 98? What are the associated 95% confidence and prediction intervals? library(ISLR2) fit &lt;- lm(mpg ~ horsepower, data = Auto) summary(fit) ## ## Call: ## lm(formula = mpg ~ horsepower, data = Auto) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.5710 -3.2592 -0.3435 2.7630 16.9240 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 39.935861 0.717499 55.66 &lt;2e-16 *** ## horsepower -0.157845 0.006446 -24.49 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.906 on 390 degrees of freedom ## Multiple R-squared: 0.6059, Adjusted R-squared: 0.6049 ## F-statistic: 599.7 on 1 and 390 DF, p-value: &lt; 2.2e-16 Yes, there is a significant relationship between predictor and response. For every unit increase in horsepower, mpg reduces by 0.16 (a negative relationship). predict(fit, data.frame(horsepower = 98), interval = &quot;confidence&quot;) ## fit lwr upr ## 1 24.46708 23.97308 24.96108 predict(fit, data.frame(horsepower = 98), interval = &quot;prediction&quot;) ## fit lwr upr ## 1 24.46708 14.8094 34.12476 Plot the response and the predictor. Use the abline() function to display the least squares regression line. plot(Auto$horsepower, Auto$mpg) abline(fit) Use the plot() function to produce diagnostic plots of the least squares regression fit. Comment on any problems you see with the fit. par(mfrow = c(2, 2)) plot(fit, cex = 0.2) The residuals show a trend with respect to the fitted values suggesting a non-linear relationship. 3.2.2 Question 9 This question involves the use of multiple linear regression on the Auto data set. Produce a scatterplot matrix which includes all of the variables in the data set. pairs(Auto, cex = 0.2) Compute the matrix of correlations between the variables using the function cor(). You will need to exclude the name variable, name which is qualitative. x &lt;- subset(Auto, select = -name) cor(x) ## mpg cylinders displacement horsepower weight ## mpg 1.0000000 -0.7776175 -0.8051269 -0.7784268 -0.8322442 ## cylinders -0.7776175 1.0000000 0.9508233 0.8429834 0.8975273 ## displacement -0.8051269 0.9508233 1.0000000 0.8972570 0.9329944 ## horsepower -0.7784268 0.8429834 0.8972570 1.0000000 0.8645377 ## weight -0.8322442 0.8975273 0.9329944 0.8645377 1.0000000 ## acceleration 0.4233285 -0.5046834 -0.5438005 -0.6891955 -0.4168392 ## year 0.5805410 -0.3456474 -0.3698552 -0.4163615 -0.3091199 ## origin 0.5652088 -0.5689316 -0.6145351 -0.4551715 -0.5850054 ## acceleration year origin ## mpg 0.4233285 0.5805410 0.5652088 ## cylinders -0.5046834 -0.3456474 -0.5689316 ## displacement -0.5438005 -0.3698552 -0.6145351 ## horsepower -0.6891955 -0.4163615 -0.4551715 ## weight -0.4168392 -0.3091199 -0.5850054 ## acceleration 1.0000000 0.2903161 0.2127458 ## year 0.2903161 1.0000000 0.1815277 ## origin 0.2127458 0.1815277 1.0000000 Use the lm() function to perform a multiple linear regression with mpg as the response and all other variables except name as the predictors. Use the summary() function to print the results. Comment on the output. For instance: Is there a relationship between the predictors and the response? Which predictors appear to have a statistically significant relationship to the response? What does the coefficient for the year variable suggest? fit &lt;- lm(mpg ~ ., data = x) summary(fit) ## ## Call: ## lm(formula = mpg ~ ., data = x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.5903 -2.1565 -0.1169 1.8690 13.0604 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -17.218435 4.644294 -3.707 0.00024 *** ## cylinders -0.493376 0.323282 -1.526 0.12780 ## displacement 0.019896 0.007515 2.647 0.00844 ** ## horsepower -0.016951 0.013787 -1.230 0.21963 ## weight -0.006474 0.000652 -9.929 &lt; 2e-16 *** ## acceleration 0.080576 0.098845 0.815 0.41548 ## year 0.750773 0.050973 14.729 &lt; 2e-16 *** ## origin 1.426141 0.278136 5.127 4.67e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.328 on 384 degrees of freedom ## Multiple R-squared: 0.8215, Adjusted R-squared: 0.8182 ## F-statistic: 252.4 on 7 and 384 DF, p-value: &lt; 2.2e-16 Yes, there is a relationship between some predictors and response, notably “displacement” (positive), “weight” (negative), “year” (positive) and “origin” (positive). The coefficient for year (which is positive \\(~0.75\\)) suggests that mpg increases by about this amount every year on average. Use the plot() function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage? par(mfrow = c(2, 2)) plot(fit, cex = 0.2) One point has high leverage, the residuals also show a trend with fitted values. Use the * and : symbols to fit linear regression models with interaction effects. Do any interactions appear to be statistically significant? summary(lm(mpg ~ . + weight:horsepower, data = x)) ## ## Call: ## lm(formula = mpg ~ . + weight:horsepower, data = x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.589 -1.617 -0.184 1.541 12.001 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.876e+00 4.511e+00 0.638 0.524147 ## cylinders -2.955e-02 2.881e-01 -0.103 0.918363 ## displacement 5.950e-03 6.750e-03 0.881 0.378610 ## horsepower -2.313e-01 2.363e-02 -9.791 &lt; 2e-16 *** ## weight -1.121e-02 7.285e-04 -15.393 &lt; 2e-16 *** ## acceleration -9.019e-02 8.855e-02 -1.019 0.309081 ## year 7.695e-01 4.494e-02 17.124 &lt; 2e-16 *** ## origin 8.344e-01 2.513e-01 3.320 0.000986 *** ## horsepower:weight 5.529e-05 5.227e-06 10.577 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.931 on 383 degrees of freedom ## Multiple R-squared: 0.8618, Adjusted R-squared: 0.859 ## F-statistic: 298.6 on 8 and 383 DF, p-value: &lt; 2.2e-16 summary(lm(mpg ~ . + acceleration:horsepower, data = x)) ## ## Call: ## lm(formula = mpg ~ . + acceleration:horsepower, data = x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.0329 -1.8177 -0.1183 1.7247 12.4870 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -32.499820 4.923380 -6.601 1.36e-10 *** ## cylinders 0.083489 0.316913 0.263 0.792350 ## displacement -0.007649 0.008161 -0.937 0.349244 ## horsepower 0.127188 0.024746 5.140 4.40e-07 *** ## weight -0.003976 0.000716 -5.552 5.27e-08 *** ## acceleration 0.983282 0.161513 6.088 2.78e-09 *** ## year 0.755919 0.048179 15.690 &lt; 2e-16 *** ## origin 1.035733 0.268962 3.851 0.000138 *** ## horsepower:acceleration -0.012139 0.001772 -6.851 2.93e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.145 on 383 degrees of freedom ## Multiple R-squared: 0.841, Adjusted R-squared: 0.8376 ## F-statistic: 253.2 on 8 and 383 DF, p-value: &lt; 2.2e-16 summary(lm(mpg ~ . + cylinders:weight, data = x)) ## ## Call: ## lm(formula = mpg ~ . + cylinders:weight, data = x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.9484 -1.7133 -0.1809 1.4530 12.4137 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.3143478 5.0076737 1.461 0.14494 ## cylinders -5.0347425 0.5795767 -8.687 &lt; 2e-16 *** ## displacement 0.0156444 0.0068409 2.287 0.02275 * ## horsepower -0.0314213 0.0126216 -2.489 0.01322 * ## weight -0.0150329 0.0011125 -13.513 &lt; 2e-16 *** ## acceleration 0.1006438 0.0897944 1.121 0.26306 ## year 0.7813453 0.0464139 16.834 &lt; 2e-16 *** ## origin 0.8030154 0.2617333 3.068 0.00231 ** ## cylinders:weight 0.0015058 0.0001657 9.088 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.022 on 383 degrees of freedom ## Multiple R-squared: 0.8531, Adjusted R-squared: 0.8501 ## F-statistic: 278.1 on 8 and 383 DF, p-value: &lt; 2.2e-16 There are at least three cases where the interactions appear to be highly significant. Try a few different transformations of the variables, such as \\(log(X)\\), \\(\\sqrt{X}\\), \\(X^2\\). Comment on your findings. Here I’ll just consider transformations for horsepower. par(mfrow = c(2, 2)) plot(Auto$horsepower, Auto$mpg, cex = 0.2) plot(log(Auto$horsepower), Auto$mpg, cex = 0.2) plot(sqrt(Auto$horsepower), Auto$mpg, cex = 0.2) plot(Auto$horsepower ^ 2, Auto$mpg, cex = 0.2) x &lt;- subset(Auto, select = -name) x$horsepower &lt;- log(x$horsepower) fit &lt;- lm(mpg ~ ., data = x) summary(fit) ## ## Call: ## lm(formula = mpg ~ ., data = x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.3115 -2.0041 -0.1726 1.8393 12.6579 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 27.254005 8.589614 3.173 0.00163 ** ## cylinders -0.486206 0.306692 -1.585 0.11372 ## displacement 0.019456 0.006876 2.830 0.00491 ** ## horsepower -9.506436 1.539619 -6.175 1.69e-09 *** ## weight -0.004266 0.000694 -6.148 1.97e-09 *** ## acceleration -0.292088 0.103804 -2.814 0.00515 ** ## year 0.705329 0.048456 14.556 &lt; 2e-16 *** ## origin 1.482435 0.259347 5.716 2.19e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.18 on 384 degrees of freedom ## Multiple R-squared: 0.837, Adjusted R-squared: 0.834 ## F-statistic: 281.6 on 7 and 384 DF, p-value: &lt; 2.2e-16 par(mfrow = c(2, 2)) plot(fit, cex = 0.2) A log transformation of horsepower appears to give a more linear relationship with mpg. 3.2.3 Question 10 This question should be answered using the Carseats data set. Fit a multiple regression model to predict Sales using Price, Urban, and US. fit &lt;- lm(Sales ~ Price + Urban + US, data = Carseats) Provide an interpretation of each coefficient in the model. Be careful—some of the variables in the model are qualitative! summary(fit) ## ## Call: ## lm(formula = Sales ~ Price + Urban + US, data = Carseats) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.9206 -1.6220 -0.0564 1.5786 7.0581 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13.043469 0.651012 20.036 &lt; 2e-16 *** ## Price -0.054459 0.005242 -10.389 &lt; 2e-16 *** ## UrbanYes -0.021916 0.271650 -0.081 0.936 ## USYes 1.200573 0.259042 4.635 4.86e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.472 on 396 degrees of freedom ## Multiple R-squared: 0.2393, Adjusted R-squared: 0.2335 ## F-statistic: 41.52 on 3 and 396 DF, p-value: &lt; 2.2e-16 Write out the model in equation form, being careful to handle the qualitative variables properly. \\[ \\textit{Sales} = 13 + -0.054 \\times \\textit{Price} + \\begin{cases} -0.022, &amp; \\text{if $\\textit{Urban}$ is Yes, $\\textit{US}$ is No} \\\\ 1.20, &amp; \\text{if $\\textit{Urban}$ is No, $\\textit{US}$ is Yes} \\\\ 1.18, &amp; \\text{if $\\textit{Urban}$ and $\\textit{US}$ is Yes} \\\\ 0, &amp; \\text{Otherwise} \\end{cases} \\] For which of the predictors can you reject the null hypothesis \\(H_0 : \\beta_j = 0\\)? Price and US (Urban shows no significant difference between “No” and “Yes”) On the basis of your response to the previous question, fit a smaller model that only uses the predictors for which there is evidence of association with the outcome. fit2 &lt;- lm(Sales ~ Price + US, data = Carseats) How well do the models in (a) and (e) fit the data? summary(fit) ## ## Call: ## lm(formula = Sales ~ Price + Urban + US, data = Carseats) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.9206 -1.6220 -0.0564 1.5786 7.0581 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13.043469 0.651012 20.036 &lt; 2e-16 *** ## Price -0.054459 0.005242 -10.389 &lt; 2e-16 *** ## UrbanYes -0.021916 0.271650 -0.081 0.936 ## USYes 1.200573 0.259042 4.635 4.86e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.472 on 396 degrees of freedom ## Multiple R-squared: 0.2393, Adjusted R-squared: 0.2335 ## F-statistic: 41.52 on 3 and 396 DF, p-value: &lt; 2.2e-16 summary(fit2) ## ## Call: ## lm(formula = Sales ~ Price + US, data = Carseats) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.9269 -1.6286 -0.0574 1.5766 7.0515 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13.03079 0.63098 20.652 &lt; 2e-16 *** ## Price -0.05448 0.00523 -10.416 &lt; 2e-16 *** ## USYes 1.19964 0.25846 4.641 4.71e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.469 on 397 degrees of freedom ## Multiple R-squared: 0.2393, Adjusted R-squared: 0.2354 ## F-statistic: 62.43 on 2 and 397 DF, p-value: &lt; 2.2e-16 anova(fit, fit2) ## Analysis of Variance Table ## ## Model 1: Sales ~ Price + Urban + US ## Model 2: Sales ~ Price + US ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 396 2420.8 ## 2 397 2420.9 -1 -0.03979 0.0065 0.9357 They have similar \\(R^2\\) and the model containing the extra variable “Urban” is non-significantly better. Using the model from (e), obtain 95% confidence intervals for the coefficient(s). confint(fit2) ## 2.5 % 97.5 % ## (Intercept) 11.79032020 14.27126531 ## Price -0.06475984 -0.04419543 ## USYes 0.69151957 1.70776632 Is there evidence of outliers or high leverage observations in the model from (e)? par(mfrow = c(2, 2)) plot(fit2, cex = 0.2) Yes, somewhat. 3.2.4 Question 11 In this problem we will investigate the t-statistic for the null hypothesis \\(H_0 : \\beta = 0\\) in simple linear regression without an intercept. To begin, we generate a predictor x and a response y as follows. set.seed(1) x &lt;- rnorm(100) y &lt;- 2 * x + rnorm(100) set.seed(1) x &lt;- rnorm(100) y &lt;- 2 * x + rnorm(100) Perform a simple linear regression of y onto x, without an intercept. Report the coefficient estimate \\(\\hat{\\beta}\\), the standard error of this coefficient estimate, and the t-statistic and p-value associated with the null hypothesis \\(H_0 : \\beta = 0\\). Comment on these results. (You can perform regression without an intercept using the command lm(y~x+0).) fit &lt;- lm(y ~ x + 0) coef(summary(fit)) ## Estimate Std. Error t value Pr(&gt;|t|) ## x 1.993876 0.1064767 18.72593 2.642197e-34 There’s a significant positive relationship between \\(y\\) and \\(x\\). \\(y\\) values are predicted to be (a little below) twice the \\(x\\) values. Now perform a simple linear regression of x onto y without an intercept, and report the coefficient estimate, its standard error, and the corresponding t-statistic and p-values associated with the null hypothesis \\(H_0 : \\beta = 0\\). Comment on these results. fit &lt;- lm(x ~ y + 0) coef(summary(fit)) ## Estimate Std. Error t value Pr(&gt;|t|) ## y 0.3911145 0.02088625 18.72593 2.642197e-34 There’s a significant positive relationship between \\(x\\) and \\(y\\). \\(x\\) values are predicted to be (a little below) half the \\(y\\) values. What is the relationship between the results obtained in (a) and (b)? Without error, the coefficients would be the inverse of each other (2 and 1/2). The t-statistic and p-values are the same. For the regression of \\(Y\\) onto \\(X\\) without an intercept, the t-statistic for \\(H_0 : \\beta = 0\\) takes the form \\(\\hat{\\beta}/SE(\\hat{\\beta})\\), where \\(\\hat{\\beta}\\) is given by (3.38), and where \\[ SE(\\hat\\beta) = \\sqrt{\\frac{\\sum_{i=1}^n(y_i - x_i\\hat\\beta)^2}{(n-1)\\sum_{i&#39;=1}^nx_{i&#39;}^2}}. \\] (These formulas are slightly different from those given in Sections 3.1.1 and 3.1.2, since here we are performing regression without an intercept.) Show algebraically, and confirm numerically in R, that the t-statistic can be written as \\[ \\frac{(\\sqrt{n-1}) \\sum_{i-1}^nx_iy_i)} {\\sqrt{(\\sum_{i=1}^nx_i^2)(\\sum_{i&#39;=1}^ny_{i&#39;}^2)-(\\sum_{i&#39;=1}^nx_{i&#39;}y_{i&#39;})^2}} \\] \\[ \\beta = \\sum_i x_i y_i / \\sum_{i&#39;} x_{i&#39;}^2 ,\\] therefore \\[\\begin{align} t &amp;= \\frac{\\sum_i x_i y_i \\sqrt{n-1} \\sqrt{\\sum_ix_i^2}} {\\sum_i x_i^2 \\sqrt{\\sum_i(y_i - x_i \\beta)^2}} \\\\ &amp;= \\frac{\\sum_i x_i y_i \\sqrt{n-1}} {\\sqrt{\\sum_ix_i^2 \\sum_i(y_i - x_i \\beta)^2}} \\\\ &amp;= \\frac{\\sum_i x_i y_i \\sqrt{n-1}} {\\sqrt{ \\sum_ix_i^2 \\sum_i(y_i^2 - 2 y_i x_i \\beta + x_i^2 \\beta^2) }} \\\\ &amp;= \\frac{\\sum_i x_i y_i \\sqrt{n-1}} {\\sqrt{ \\sum_ix_i^2 \\sum_iy_i^2 - \\beta \\sum_ix_i^2 (2 \\sum_i y_i x_i -\\beta \\sum_i x_i^2) }} \\\\ &amp;= \\frac{\\sum_i x_i y_i \\sqrt{n-1}} {\\sqrt{ \\sum_ix_i^2 \\sum_iy_i^2 - \\sum_i x_i y_i (2 \\sum_i y_i x_i - \\sum_i x_i y_i) }} \\\\ &amp;= \\frac{\\sum_i x_i y_i \\sqrt{n-1}} {\\sqrt{\\sum_ix_i^2 \\sum_iy_i^2 - (\\sum_i x_i y_i)^2}} \\\\ \\end{align}\\] We can show this numerically in R by computing \\(t\\) using the above equation. n &lt;- length(x) sqrt(n - 1) * sum(x * y) / sqrt(sum(x ^ 2) * sum(y ^ 2) - sum(x * y) ^ 2) ## [1] 18.72593 Using the results from (d), argue that the t-statistic for the regression of y onto x is the same as the t-statistic for the regression of x onto y. Swapping \\(x_i\\) for \\(y_i\\) in the formula for \\(t\\) will give the same result. In R, show that when regression is performed with an intercept, the t-statistic for \\(H_0 : \\beta_1 = 0\\) is the same for the regression of y onto x as it is for the regression of x onto y. coef(summary(lm(y ~ x))) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.03769261 0.09698729 -0.3886346 6.983896e-01 ## x 1.99893961 0.10772703 18.5555993 7.723851e-34 coef(summary(lm(x ~ y))) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.03880394 0.04266144 0.9095787 3.652764e-01 ## y 0.38942451 0.02098690 18.5555993 7.723851e-34 3.2.5 Question 12 This problem involves simple linear regression without an intercept. Recall that the coefficient estimate \\(\\hat{\\beta}\\) for the linear regression of \\(Y\\) onto \\(X\\) without an intercept is given by (3.38). Under what circumstance is the coefficient estimate for the regression of \\(X\\) onto \\(Y\\) the same as the coefficient estimate for the regression of \\(Y\\) onto \\(X\\)? \\[ \\hat\\beta = \\sum_i x_iy_i / \\sum_{i&#39;} x_{i&#39;}^2 \\] The coefficient for the regression of X onto Y swaps the \\(x\\) and \\(y\\) variables: \\[ \\hat\\beta = \\sum_i x_iy_i / \\sum_{i&#39;} y_{i&#39;}^2 \\] So they are the same when \\(\\sum_{i} x_{i}^2 = \\sum_{i} y_{i}^2\\) Generate an example in R with \\(n = 100\\) observations in which the coefficient estimate for the regression of \\(X\\) onto \\(Y\\) is different from the coefficient estimate for the regression of \\(Y\\) onto \\(X\\). x &lt;- rnorm(100) y &lt;- 2 * x + rnorm(100, 0, 0.1) c(sum(x^2), sum(y^2)) ## [1] 105.9889 429.4924 c(coef(lm(y ~ x))[2], coef(lm(x ~ y))[2]) ## x y ## 2.0106218 0.4962439 Generate an example in R with \\(n = 100\\) observations in which the coefficient estimate for the regression of \\(X\\) onto \\(Y\\) is the same as the coefficient estimate for the regression of \\(Y\\) onto \\(X\\). x &lt;- rnorm(100) y &lt;- x + rnorm(100, 0, 0.1) c(sum(x^2), sum(y^2)) ## [1] 135.5844 134.5153 c(coef(lm(y ~ x))[2], coef(lm(x ~ y))[2]) ## x y ## 0.9925051 1.0006765 3.2.6 Question 13 In this exercise you will create some simulated data and will fit simple linear regression models to it. Make sure to use set.seed(1) prior to starting part (a) to ensure consistent results. set.seed(1) Using the rnorm() function, create a vector, x, containing 100 observations drawn from a \\(N(0, 1)\\) distribution. This represents a feature, \\(X\\). x &lt;- rnorm(100, 0, 1) Using the rnorm() function, create a vector, eps, containing 100 observations drawn from a \\(N(0, 0.25)\\) distribution—a normal distribution with mean zero and variance 0.25. eps &lt;- rnorm(100, 0, sqrt(0.25)) Using x and eps, generate a vector y according to the model \\[Y = -1 + 0.5X + \\epsilon\\] What is the length of the vector y? What are the values of \\(\\beta_0\\) and \\(\\beta_1\\) in this linear model? y &lt;- -1 + 0.5 * x + eps length(y) ## [1] 100 \\(\\beta_0 = -1\\) and \\(\\beta_1 = 0.5\\) Create a scatterplot displaying the relationship between x and y. Comment on what you observe. plot(x, y) There is a linear relationship between \\(x\\) and \\(y\\) (with some error). Fit a least squares linear model to predict y using x. Comment on the model obtained. How do \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) compare to \\(\\beta_0\\) and \\(\\beta_1\\)? fit &lt;- lm(y ~ x) summary(fit) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.93842 -0.30688 -0.06975 0.26970 1.17309 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.01885 0.04849 -21.010 &lt; 2e-16 *** ## x 0.49947 0.05386 9.273 4.58e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4814 on 98 degrees of freedom ## Multiple R-squared: 0.4674, Adjusted R-squared: 0.4619 ## F-statistic: 85.99 on 1 and 98 DF, p-value: 4.583e-15 \\(\\beta_0\\) and \\(\\beta_1\\) are close to their population values. Display the least squares line on the scatterplot obtained in (d). Draw the population regression line on the plot, in a different color. Use the legend() command to create an appropriate legend. plot(x, y) abline(fit) abline(-1, 0.5, col = &quot;red&quot;, lty = 2) legend(&quot;topleft&quot;, c(&quot;model fit&quot;, &quot;population regression&quot;), col = c(&quot;black&quot;, &quot;red&quot;), lty = c(1, 2) ) Now fit a polynomial regression model that predicts y using x and x^2. Is there evidence that the quadratic term improves the model fit? Explain your answer. fit2 &lt;- lm(y ~ poly(x, 2)) anova(fit2, fit) ## Analysis of Variance Table ## ## Model 1: y ~ poly(x, 2) ## Model 2: y ~ x ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 97 22.257 ## 2 98 22.709 -1 -0.45163 1.9682 0.1638 There is no evidence for an improved fit, since the F-test is non-significant. Repeat (a)–(f) after modifying the data generation process in such a way that there is less noise in the data. The model (3.39) should remain the same. You can do this by decreasing the variance of the normal distribution used to generate the error term \\(\\epsilon\\) in (b). Describe your results. x &lt;- rnorm(100, 0, 1) y &lt;- -1 + 0.5 * x + rnorm(100, 0, sqrt(0.05)) fit2 &lt;- lm(y ~ x) summary(fit2) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.61308 -0.12553 -0.00391 0.15199 0.41332 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.98917 0.02216 -44.64 &lt;2e-16 *** ## x 0.52375 0.02152 24.33 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2215 on 98 degrees of freedom ## Multiple R-squared: 0.858, Adjusted R-squared: 0.8565 ## F-statistic: 592.1 on 1 and 98 DF, p-value: &lt; 2.2e-16 plot(x, y) abline(fit2) abline(-1, 0.5, col = &quot;red&quot;, lty = 2) legend(&quot;topleft&quot;, c(&quot;model fit&quot;, &quot;population regression&quot;), col = c(&quot;black&quot;, &quot;red&quot;), lty = c(1, 2) ) The data shows less variability and the \\(R^2\\) is higher. Repeat (a)–(f) after modifying the data generation process in such a way that there is more noise in the data. The model (3.39) should remain the same. You can do this by increasing the variance of the normal distribution used to generate the error term \\(\\epsilon\\) in (b). Describe your results. x &lt;- rnorm(100, 0, 1) y &lt;- -1 + 0.5 * x + rnorm(100, 0, 1) fit3 &lt;- lm(y ~ x) summary(fit3) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.51014 -0.60549 0.02065 0.70483 2.08980 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.04745 0.09676 -10.825 &lt; 2e-16 *** ## x 0.42505 0.08310 5.115 1.56e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9671 on 98 degrees of freedom ## Multiple R-squared: 0.2107, Adjusted R-squared: 0.2027 ## F-statistic: 26.16 on 1 and 98 DF, p-value: 1.56e-06 plot(x, y) abline(fit3) abline(-1, 0.5, col = &quot;red&quot;, lty = 2) legend(&quot;topleft&quot;, c(&quot;model fit&quot;, &quot;population regression&quot;), col = c(&quot;black&quot;, &quot;red&quot;), lty = c(1, 2) ) The data shows more variability. The \\(R^2\\) is lower. What are the confidence intervals for \\(\\beta_0\\) and \\(\\beta_1\\) based on the original data set, the noisier data set, and the less noisy data set? Comment on your results. confint(fit) ## 2.5 % 97.5 % ## (Intercept) -1.1150804 -0.9226122 ## x 0.3925794 0.6063602 confint(fit2) ## 2.5 % 97.5 % ## (Intercept) -1.033141 -0.9451916 ## x 0.481037 0.5664653 confint(fit3) ## 2.5 % 97.5 % ## (Intercept) -1.2394772 -0.8554276 ## x 0.2601391 0.5899632 The confidence intervals for the coefficients are smaller when there is less error. 3.2.7 Question 14 This problem focuses on the collinearity problem. Perform the following commands in R : &gt; set.seed(1) &gt; x1 &lt;- runif(100) &gt; x2 &lt;- 0.5 * x1 + rnorm(100) / 10 &gt; y &lt;- 2 + 2 * x1 + 0.3 * x2 + rnorm(100) The last line corresponds to creating a linear model in which y is a function of x1 and x2. Write out the form of the linear model. What are the regression coefficients? set.seed(1) x1 &lt;- runif(100) x2 &lt;- 0.5 * x1 + rnorm(100) / 10 y &lt;- 2 + 2 * x1 + 0.3 * x2 + rnorm(100) The model is of the form: \\[Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\epsilon\\] The coefficients are \\(\\beta_0 = 2\\), \\(\\beta_1 = 2\\), \\(\\beta_3 = 0.3\\). What is the correlation between x1 and x2? Create a scatterplot displaying the relationship between the variables. cor(x1, x2) ## [1] 0.8351212 plot(x1, x2) Using this data, fit a least squares regression to predict y using x1 and x2. Describe the results obtained. What are \\(\\hat\\beta_0\\), \\(\\hat\\beta_1\\), and \\(\\hat\\beta_2\\)? How do these relate to the true \\(\\beta_0\\), \\(\\beta_1\\), and _2$? Can you reject the null hypothesis \\(H_0 : \\beta_1\\) = 0$? How about the null hypothesis \\(H_0 : \\beta_2 = 0\\)? summary(lm(y ~ x1 + x2)) ## ## Call: ## lm(formula = y ~ x1 + x2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.8311 -0.7273 -0.0537 0.6338 2.3359 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.1305 0.2319 9.188 7.61e-15 *** ## x1 1.4396 0.7212 1.996 0.0487 * ## x2 1.0097 1.1337 0.891 0.3754 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.056 on 97 degrees of freedom ## Multiple R-squared: 0.2088, Adjusted R-squared: 0.1925 ## F-statistic: 12.8 on 2 and 97 DF, p-value: 1.164e-05 \\(\\hat\\beta_0 = 2.13\\), \\(\\hat\\beta_1 = 1.43\\), and \\(\\hat\\beta_2 = 1.01\\). These are relatively poor estimates of the true values. We can reject the hypothesis that \\(H_0 : \\beta_1\\) at a p-value of 0.05 (just about). We cannot reject the hypothesis that \\(H_0 : \\beta_2 = 0\\). Now fit a least squares regression to predict y using only x1. Comment on your results. Can you reject the null hypothesis \\(H 0 : \\beta_1 = 0\\)? summary(lm(y ~ x1)) ## ## Call: ## lm(formula = y ~ x1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.89495 -0.66874 -0.07785 0.59221 2.45560 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.1124 0.2307 9.155 8.27e-15 *** ## x1 1.9759 0.3963 4.986 2.66e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.055 on 98 degrees of freedom ## Multiple R-squared: 0.2024, Adjusted R-squared: 0.1942 ## F-statistic: 24.86 on 1 and 98 DF, p-value: 2.661e-06 We can reject \\(H_0 : \\beta_1 = 0\\). The p-value is much more significant for \\(\\beta_1\\) compared to when x2 is included in the model. Now fit a least squares regression to predict y using only x2. Comment on your results. Can you reject the null hypothesis \\(H_0 : \\beta_1 = 0\\)? summary(lm(y ~ x2)) ## ## Call: ## lm(formula = y ~ x2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.62687 -0.75156 -0.03598 0.72383 2.44890 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.3899 0.1949 12.26 &lt; 2e-16 *** ## x2 2.8996 0.6330 4.58 1.37e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.072 on 98 degrees of freedom ## Multiple R-squared: 0.1763, Adjusted R-squared: 0.1679 ## F-statistic: 20.98 on 1 and 98 DF, p-value: 1.366e-05 Similarly, we can reject \\(H_0 : \\beta_2 = 0\\). The p-value is much more significant for \\(\\beta_2\\) compared to when x1 is included in the model. Do the results obtained in (c)–(e) contradict each other? Explain your answer. No they do not contradict each other. Both x1 and x2 individually are capable of explaining much of the variation observed in y, however since they are correlated, it is very difficult to tease apart their separate contributions. Now suppose we obtain one additional observation, which was unfortunately mismeasured. &gt; x1 &lt;- c(x1, 0.1) &gt; x2 &lt;- c(x2, 0.8) &gt; y &lt;- c(y, 6) Re-fit the linear models from (c) to (e) using this new data. What effect does this new observation have on the each of the models? In each model, is this observation an outlier? A high-leverage point? Both? Explain your answers. x1 &lt;- c(x1 , 0.1) x2 &lt;- c(x2 , 0.8) y &lt;- c(y ,6) summary(lm(y ~ x1 + x2)) ## ## Call: ## lm(formula = y ~ x1 + x2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.73348 -0.69318 -0.05263 0.66385 2.30619 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.2267 0.2314 9.624 7.91e-16 *** ## x1 0.5394 0.5922 0.911 0.36458 ## x2 2.5146 0.8977 2.801 0.00614 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.075 on 98 degrees of freedom ## Multiple R-squared: 0.2188, Adjusted R-squared: 0.2029 ## F-statistic: 13.72 on 2 and 98 DF, p-value: 5.564e-06 summary(lm(y ~ x1)) ## ## Call: ## lm(formula = y ~ x1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.8897 -0.6556 -0.0909 0.5682 3.5665 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.2569 0.2390 9.445 1.78e-15 *** ## x1 1.7657 0.4124 4.282 4.29e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.111 on 99 degrees of freedom ## Multiple R-squared: 0.1562, Adjusted R-squared: 0.1477 ## F-statistic: 18.33 on 1 and 99 DF, p-value: 4.295e-05 summary(lm(y ~ x2)) ## ## Call: ## lm(formula = y ~ x2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.64729 -0.71021 -0.06899 0.72699 2.38074 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.3451 0.1912 12.264 &lt; 2e-16 *** ## x2 3.1190 0.6040 5.164 1.25e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.074 on 99 degrees of freedom ## Multiple R-squared: 0.2122, Adjusted R-squared: 0.2042 ## F-statistic: 26.66 on 1 and 99 DF, p-value: 1.253e-06 par(mfrow = c(2, 2)) plot(lm(y ~ x1 + x2), cex = 0.2) par(mfrow = c(2, 2)) plot(lm(y ~ x1), cex = 0.2) par(mfrow = c(2, 2)) plot(lm(y ~ x2), cex = 0.2) In the first model (with both predictors), the new point has very high leverage (since it is an outlier in terms of the joint x1 and x2 distribution), however it is not an outlier. In the model that includes x1, it is an outlier but does not have high leverage. In the model that includes x2, it has high leverage but is not an outlier. It is useful to consider the scatterplot of x1 and x2. plot(x1, x2) points(0.1, 0.8, col = &quot;red&quot;, pch = 19) 3.2.8 Question 15 This problem involves the Boston data set, which we saw in the lab for this chapter. We will now try to predict per capita crime rate using the other variables in this data set. In other words, per capita crime rate is the response, and the other variables are the predictors. We are trying to predict crim. pred &lt;- subset(Boston, select = -crim) For each predictor, fit a simple linear regression model to predict the response. Describe your results. In which of the models is there a statistically significant association between the predictor and the response? Create some plots to back up your assertions. fits &lt;- lapply(pred, function(x) lm(Boston$crim ~ x)) printCoefmat(do.call(rbind, lapply(fits, function(x) coef(summary(x))[2, ]))) ## Estimate Std. Error t value Pr(&gt;|t|) ## zn -0.0739350 0.0160946 -4.5938 5.506e-06 *** ## indus 0.5097763 0.0510243 9.9908 &lt; 2.2e-16 *** ## chas -1.8927766 1.5061155 -1.2567 0.2094 ## nox 31.2485312 2.9991904 10.4190 &lt; 2.2e-16 *** ## rm -2.6840512 0.5320411 -5.0448 6.347e-07 *** ## age 0.1077862 0.0127364 8.4628 2.855e-16 *** ## dis -1.5509017 0.1683300 -9.2135 &lt; 2.2e-16 *** ## rad 0.6179109 0.0343318 17.9982 &lt; 2.2e-16 *** ## tax 0.0297423 0.0018474 16.0994 &lt; 2.2e-16 *** ## ptratio 1.1519828 0.1693736 6.8014 2.943e-11 *** ## lstat 0.5488048 0.0477610 11.4907 &lt; 2.2e-16 *** ## medv -0.3631599 0.0383902 -9.4597 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 There are significant associations for all predictors with the exception of chas when fitting separate linear models. For example, consider the following plot representing the third model plot(Boston$rm, Boston$crim) abline(fits[[5]]) Fit a multiple regression model to predict the response using all of the predictors. Describe your results. For which predictors can we reject the null hypothesis \\(H_0 : \\beta_j = 0\\)? mfit &lt;- lm(crim ~ ., data = Boston) summary(mfit) ## ## Call: ## lm(formula = crim ~ ., data = Boston) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.534 -2.248 -0.348 1.087 73.923 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13.7783938 7.0818258 1.946 0.052271 . ## zn 0.0457100 0.0187903 2.433 0.015344 * ## indus -0.0583501 0.0836351 -0.698 0.485709 ## chas -0.8253776 1.1833963 -0.697 0.485841 ## nox -9.9575865 5.2898242 -1.882 0.060370 . ## rm 0.6289107 0.6070924 1.036 0.300738 ## age -0.0008483 0.0179482 -0.047 0.962323 ## dis -1.0122467 0.2824676 -3.584 0.000373 *** ## rad 0.6124653 0.0875358 6.997 8.59e-12 *** ## tax -0.0037756 0.0051723 -0.730 0.465757 ## ptratio -0.3040728 0.1863598 -1.632 0.103393 ## lstat 0.1388006 0.0757213 1.833 0.067398 . ## medv -0.2200564 0.0598240 -3.678 0.000261 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.46 on 493 degrees of freedom ## Multiple R-squared: 0.4493, Adjusted R-squared: 0.4359 ## F-statistic: 33.52 on 12 and 493 DF, p-value: &lt; 2.2e-16 There are now only significant associations for zn, dis, rad, black and medv. How do your results from (a) compare to your results from (b)? Create a plot displaying the univariate regression coefficients from (a) on the \\(x\\)-axis, and the multiple regression coefficients from (b) on the \\(y\\)-axis. That is, each predictor is displayed as a single point in the plot. Its coefficient in a simple linear regression model is shown on the x-axis, and its coefficient estimate in the multiple linear regression model is shown on the y-axis. The results from (b) show reduced significance compared to the models fit in (a). plot(sapply(fits, function(x) coef(x)[2]), coef(mfit)[-1], xlab = &quot;Univariate regression&quot;, ylab = &quot;multiple regression&quot;) The estimated coefficients differ (in particular the estimated coefficient for nox is dramatically different) between the two modelling strategies. Is there evidence of non-linear association between any of the predictors and the response? To answer this question, for each predictor X, fit a model of the form \\[ Y = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\beta_3X^3 + \\epsilon \\] pred &lt;- subset(pred, select = -chas) fits &lt;- lapply(names(pred), function(p) { f &lt;- paste0(&quot;crim ~ poly(&quot;, p, &quot;, 3)&quot;) lm(as.formula(f), data = Boston) }) for (fit in fits) printCoefmat(coef(summary(fit))) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.61352 0.37219 9.7088 &lt; 2.2e-16 *** ## poly(zn, 3)1 -38.74984 8.37221 -4.6284 4.698e-06 *** ## poly(zn, 3)2 23.93983 8.37221 2.8594 0.004421 ** ## poly(zn, 3)3 -10.07187 8.37221 -1.2030 0.229539 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.6135 0.3300 10.9501 &lt; 2.2e-16 *** ## poly(indus, 3)1 78.5908 7.4231 10.5873 &lt; 2.2e-16 *** ## poly(indus, 3)2 -24.3948 7.4231 -3.2863 0.001086 ** ## poly(indus, 3)3 -54.1298 7.4231 -7.2920 1.196e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.61352 0.32157 11.2370 &lt; 2.2e-16 *** ## poly(nox, 3)1 81.37202 7.23361 11.2492 &lt; 2.2e-16 *** ## poly(nox, 3)2 -28.82859 7.23361 -3.9854 7.737e-05 *** ## poly(nox, 3)3 -60.36189 7.23361 -8.3446 6.961e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.6135 0.3703 9.7584 &lt; 2.2e-16 *** ## poly(rm, 3)1 -42.3794 8.3297 -5.0878 5.128e-07 *** ## poly(rm, 3)2 26.5768 8.3297 3.1906 0.001509 ** ## poly(rm, 3)3 -5.5103 8.3297 -0.6615 0.508575 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.61352 0.34852 10.3683 &lt; 2.2e-16 *** ## poly(age, 3)1 68.18201 7.83970 8.6970 &lt; 2.2e-16 *** ## poly(age, 3)2 37.48447 7.83970 4.7814 2.291e-06 *** ## poly(age, 3)3 21.35321 7.83970 2.7237 0.00668 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.61352 0.32592 11.0870 &lt; 2.2e-16 *** ## poly(dis, 3)1 -73.38859 7.33148 -10.0101 &lt; 2.2e-16 *** ## poly(dis, 3)2 56.37304 7.33148 7.6892 7.870e-14 *** ## poly(dis, 3)3 -42.62188 7.33148 -5.8135 1.089e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.61352 0.29707 12.1639 &lt; 2.2e-16 *** ## poly(rad, 3)1 120.90745 6.68240 18.0934 &lt; 2.2e-16 *** ## poly(rad, 3)2 17.49230 6.68240 2.6177 0.009121 ** ## poly(rad, 3)3 4.69846 6.68240 0.7031 0.482314 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.61352 0.30468 11.8599 &lt; 2.2e-16 *** ## poly(tax, 3)1 112.64583 6.85371 16.4358 &lt; 2.2e-16 *** ## poly(tax, 3)2 32.08725 6.85371 4.6817 3.665e-06 *** ## poly(tax, 3)3 -7.99681 6.85371 -1.1668 0.2439 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.61352 0.36105 10.0084 &lt; 2.2e-16 *** ## poly(ptratio, 3)1 56.04523 8.12158 6.9008 1.565e-11 *** ## poly(ptratio, 3)2 24.77482 8.12158 3.0505 0.002405 ** ## poly(ptratio, 3)3 -22.27974 8.12158 -2.7433 0.006301 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.61352 0.33917 10.6540 &lt;2e-16 *** ## poly(lstat, 3)1 88.06967 7.62944 11.5434 &lt;2e-16 *** ## poly(lstat, 3)2 15.88816 7.62944 2.0825 0.0378 * ## poly(lstat, 3)3 -11.57402 7.62944 -1.5170 0.1299 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.61352 0.29203 12.374 &lt; 2.2e-16 *** ## poly(medv, 3)1 -75.05761 6.56915 -11.426 &lt; 2.2e-16 *** ## poly(medv, 3)2 88.08621 6.56915 13.409 &lt; 2.2e-16 *** ## poly(medv, 3)3 -48.03343 6.56915 -7.312 1.047e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Yes there is strong evidence for many variables having non-linear associations. In many cases, the addition of a cubic term is significant (indus, nox, age, dis, ptratio and medv). In other cases although the cubic terms is not significant, the squared term is (zn, rm, rad, tax, lstat). In only one case, black is there no evidence for a non-linear relationship. "],["classification.html", "4 Classification 4.1 Conceptual 4.2 Applied", " 4 Classification 4.1 Conceptual 4.1.1 Question 1 Using a little bit of algebra, prove that (4.2) is equivalent to (4.3). In other words, the logistic function representation and logit representation for the logistic regression model are equivalent. We need to show that \\[ p(X) = \\frac{e^{\\beta_0 + \\beta_1X}}{1 + e^{\\beta_0 + \\beta_1X}} \\] is equivalent to \\[ \\frac{p(X)}{1-p(X)} = e^{\\beta_0 + \\beta_1X} \\] Letting \\(x = e^{\\beta_0 + \\beta_1X}\\) \\[\\begin{align} \\frac{P(X)}{1-p(X)} &amp;= \\frac{\\frac{x}{1 + x}} {1 - \\frac{x}{1 + x}} \\\\ &amp;= \\frac{\\frac{x}{1 + x}} {\\frac{1}{1 + x}} \\\\ &amp;= x \\end{align}\\] 4.1.2 Question 2 It was stated in the text that classifying an observation to the class for which (4.12) is largest is equivalent to classifying an observation to the class for which (4.13) is largest. Prove that this is the case. In other words, under the assumption that the observations in the \\(k\\)th class are drawn from a \\(N(\\mu_k,\\sigma^2)\\) distribution, the Bayes’ classifier assigns an observation to the class for which the discriminant function is maximized. 4.12 is \\[ p_k(x) = \\frac{\\pi_k\\frac{1}{\\sqrt{2\\pi\\sigma}} \\exp(-\\frac{1}{2\\sigma^2}(x - \\mu_k)^2)} {\\sum_{l=1}^k \\pi_l\\frac{1}{\\sqrt{2\\pi\\sigma}} \\exp(-\\frac{1}{2\\sigma^2}(x - \\mu_l)^2)} \\] and the discriminant function is \\[ \\delta_k(x) = x.\\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma_2} + \\log(\\pi_k) \\] Since \\(\\sigma^2\\) is constant \\[ p_k(x) = \\frac{\\pi_k \\exp\\left(-\\frac{1}{2\\sigma^2}(x - \\mu_k)^2\\right)} {\\sum_{l=1}^k \\pi_l \\exp\\left(-\\frac{1}{2\\sigma^2}(x - \\mu_l)^2\\right)} \\] Maximizing \\(p_k(x)\\) also maximizes any monotonic function of \\(p_k(X)\\), and therefore, we can consider maximizing \\(\\log(p_K(X))\\) \\[ \\log(p_k(x)) = \\log(\\pi_k) - \\frac{1}{2\\sigma^2}(x - \\mu_k)^2 - \\log\\left(\\sum_{l=1}^k \\pi_l \\exp\\left(-\\frac{1}{2\\sigma^2}(x - \\mu_l)^2\\right)\\right) \\] Remember that we are maximizing over \\(k\\), and since the last term does not vary with \\(k\\) it can be ignored. So we just need to maximize \\[\\begin{align} f &amp;= \\log(\\pi_k) - \\frac{1}{2\\sigma^2} (x^2 - 2x\\mu_k + \\mu_k^2) \\\\ &amp;= \\log(\\pi_k) - \\frac{x^2}{2\\sigma^2} + \\frac{x\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} \\\\ \\end{align}\\] Since \\(\\frac{x^2}{2\\sigma^2}\\) is also independent of \\(k\\), we just need to maximize \\[ \\log(\\pi_k) + \\frac{x\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} \\] 4.1.3 Question 3 This problem relates to the QDA model, in which the observations within each class are drawn from a normal distribution with a class-specific mean vector and a class specific covariance matrix. We consider the simple case where \\(p = 1\\); i.e. there is only one feature. Suppose that we have \\(K\\) classes, and that if an observation belongs to the \\(k\\)th class then \\(X\\) comes from a one-dimensional normal distribution, \\(X \\sim N(\\mu_k,\\sigma^2)\\). Recall that the density function for the one-dimensional normal distribution is given in (4.16). Prove that in this case, the Bayes classifier is not linear. Argue that it is in fact quadratic. Hint: For this problem, you should follow the arguments laid out in Section 4.4.1, but without making the assumption that \\(\\sigma_1^2 = ... = \\sigma_K^2\\). As above, \\[ p_k(x) = \\frac{\\pi_k\\frac{1}{\\sqrt{2\\pi\\sigma_k}} \\exp(-\\frac{1}{2\\sigma_k^2}(x - \\mu_k)^2)} {\\sum_{l=1}^k \\pi_l\\frac{1}{\\sqrt{2\\pi\\sigma_l}} \\exp(-\\frac{1}{2\\sigma_l^2}(x - \\mu_l)^2)} \\] Now lets derive the Bayes classifier, without assuming \\(\\sigma_1^2 = ... = \\sigma_K^2\\) Maximizing \\(p_k(x)\\) also maximizes any monotonic function of \\(p_k(X)\\), and therefore, we can consider maximizing \\(\\log(p_K(X))\\) \\[ \\log(p_k(x)) = \\log(\\pi_k) + \\log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma_k}}\\right) - \\frac{1}{2\\sigma_k^2}(x - \\mu_k)^2 - \\log\\left(\\sum_{l=1}^k \\frac{1}{\\sqrt{2\\pi\\sigma_l}} \\pi_l \\exp\\left(-\\frac{1}{2\\sigma_l^2}(x - \\mu_l)^2\\right)\\right) \\] Remember that we are maximizing over \\(k\\), and since the last term does not vary with \\(k\\) it can be ignored. So we just need to maximize \\[\\begin{align} f &amp;= \\log(\\pi_k) + \\log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma_k}}\\right) - \\frac{1}{2\\sigma_k^2}(x - \\mu_k)^2 \\\\ &amp;= \\log(\\pi_k) + \\log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma_k}}\\right) - \\frac{x^2}{2\\sigma_k^2} + \\frac{x\\mu_k}{\\sigma_k^2} - \\frac{\\mu_k^2}{2\\sigma_k^2} \\\\ \\end{align}\\] However, unlike in Q2, \\(\\frac{x^2}{2\\sigma_k^2}\\) is not independent of \\(k\\), so we retain the term with \\(x^2\\), hence \\(f\\), the Bayes’ classifier, is a quadratic function of \\(x\\). 4.1.4 Question 4 When the number of features \\(p\\) is large, there tends to be a deterioration in the performance of KNN and other local approaches that perform prediction using only observations that are near the test observation for which a prediction must be made. This phenomenon is known as the curse of dimensionality, and it ties into the fact that non-parametric approaches often perform poorly when \\(p\\) is large. We will now investigate this curse. Suppose that we have a set of observations, each with measurements on \\(p = 1\\) feature, \\(X\\). We assume that \\(X\\) is uniformly (evenly) distributed on \\([0, 1]\\). Associated with each observation is a response value. Suppose that we wish to predict a test observation’s response using only observations that are within 10% of the range of \\(X\\) closest to that test observation. For instance, in order to predict the response for a test observation with \\(X = 0.6\\), we will use observations in the range \\([0.55, 0.65]\\). On average, what fraction of the available observations will we use to make the prediction? For values in \\(0...0.05\\), we use less than 10% of observations (between 5% and 10%, 7.5% on average), similarly with values in \\(0.95...1\\). For values in \\(0.05...0.95\\) we use 10% of available observations. The (weighted) average is then \\(7.5 \\times 0.1 + 10 \\times 0.9 = 9.75\\%\\). Now suppose that we have a set of observations, each with measurements on \\(p = 2\\) features, \\(X_1\\) and \\(X_2\\). We assume that \\((X_1, X_2)\\) are uniformly distributed on \\([0, 1] \\times [0, 1]\\). We wish to predict a test observation’s response using only observations that are within 10% of the range of \\(X_1\\) and within 10% of the range of \\(X_2\\) closest to that test observation. For instance, in order to predict the response for a test observation with \\(X_1 = 0.6\\) and \\(X_2 = 0.35\\), we will use observations in the range \\([0.55, 0.65]\\) for \\(X_1\\) and in the range \\([0.3, 0.4]\\) for \\(X_2\\). On average, what fraction of the available observations will we use to make the prediction? Since we need the observation to be within range for \\(X_1\\) and \\(X_2\\) we square 9.75% = \\(0.0975^2 \\times 100 = 0.95\\%\\) Now suppose that we have a set of observations on \\(p = 100\\) features. Again the observations are uniformly distributed on each feature, and again each feature ranges in value from 0 to 1. We wish to predict a test observation’s response using observations within the 10% of each feature’s range that is closest to that test observation. What fraction of the available observations will we use to make the prediction? Similar to above, we use: \\(0.0975^{100} \\times 100 = 8 \\times 10^{-100}\\%\\), essentially zero. Using your answers to parts (a)–(c), argue that a drawback of KNN when \\(p\\) is large is that there are very few training observations “near” any given test observation. As \\(p\\) increases, the fraction of observations near any given point rapidly approaches zero. For instance, even if you use 50% of the nearest observations for each \\(p\\), with \\(p = 10\\), only \\(0.5^{10} \\times 100 \\approx 0.1\\%\\) points are “near”. Now suppose that we wish to make a prediction for a test observation by creating a \\(p\\)-dimensional hypercube centered around the test observation that contains, on average, 10% of the training observations. For \\(p = 1,2,\\) and \\(100\\), what is the length of each side of the hypercube? Comment on your answer. Note: A hypercube is a generalization of a cube to an arbitrary number of dimensions. When \\(p = 1\\), a hypercube is simply a line segment, when \\(p = 2\\) it is a square, and when \\(p = 100\\) it is a 100-dimensional cube. When \\(p = 1\\), clearly the length is 0.1. When \\(p = 2\\), we need the value \\(l\\) such that \\(l^2 = 0.1\\), so \\(l = \\sqrt{0.1} = 0.32\\). When \\(p = n\\), \\(l = 0.1^{1/n}\\), so in the case of \\(n = 100\\), \\(l = 0.98\\). Therefore, the length of each side of the hypercube rapidly approaches 1 (or 100%) of the range of each \\(p\\). 4.1.5 Question 5 We now examine the differences between LDA and QDA. If the Bayes decision boundary is linear, do we expect LDA or QDA to perform better on the training set? On the test set? QDA, being a more flexible model, will always perform better on the training set, but LDA would be expected to perform better on the test set. If the Bayes decision boundary is non-linear, do we expect LDA or QDA to perform better on the training set? On the test set? QDA, being a more flexible model, will perform better on the training set, and we would hope that extra flexibility translates to a better fit on the test set. In general, as the sample size \\(n\\) increases, do we expect the test prediction accuracy of QDA relative to LDA to improve, decline, or be unchanged? Why? As \\(n\\) increases, we would expect the prediction accuracy of QDA relative to LDA to improve as there is more data to fit to subtle effects in the data. True or False: Even if the Bayes decision boundary for a given problem is linear, we will probably achieve a superior test error rate using QDA rather than LDA because QDA is flexible enough to model a linear decision boundary. Justify your answer. False. QDA can overfit leading to poorer test performance. 4.1.6 Question 6 Suppose we collect data for a group of students in a statistics class with variables \\(X_1 =\\) hours studied, \\(X_2 =\\) undergrad GPA, and \\(Y =\\) receive an A. We fit a logistic regression and produce estimated coefficient, \\(\\hat\\beta_0 = -6\\), \\(\\hat\\beta_1 = 0.05\\), \\(\\hat\\beta_2 = 1\\). Estimate the probability that a student who studies for 40h and has an undergrad GPA of 3.5 gets an A in the class. The logistic model is: \\[ \\log\\left(\\frac{p(X)}{1-p(x)}\\right) = -6 + 0.05X_1 + X_2 \\] or \\[ p(X) = \\frac{e^{-6 + 0.05X_1 + X_2}}{1 + e^{-6 + 0.05X_1 + X_2}} \\] when \\(X_1 = 40\\) and \\(X_2 = 3.5\\), \\(p(X) = 0.38\\) How many hours would the student in part (a) need to study to have a 50% chance of getting an A in the class? We would like to solve for \\(X_1\\) where \\(p(X) = 0.5\\). Taking the first equation above, we need to solve \\(0 = −6 + 0.05X_1 + 3.5\\), so \\(X_1 = 50\\) hours. 4.1.7 Question 7 Suppose that we wish to predict whether a given stock will issue a dividend this year (“Yes” or “No”) based on \\(X\\), last year’s percent profit. We examine a large number of companies and discover that the mean value of \\(X\\) for companies that issued a dividend was \\(\\bar{X} = 10\\), while the mean for those that didn’t was \\(\\bar{X} = 0\\). In addition, the variance of \\(X\\) for these two sets of companies was \\(\\hat{\\sigma}^2 = 36\\). Finally, 80% of companies issued dividends. Assuming that \\(X\\) follows a normal distribution, predict the probability that a company will issue a dividend this year given that its percentage profit was \\(X = 4\\) last year. Hint: Recall that the density function for a normal random variable is \\(f(x) =\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-(x-\\mu)^2/2\\sigma^2}\\). You will need to use Bayes’ theorem. Value \\(v\\) for companies (D) issuing a dividend = \\(v_D \\sim \\mathcal{N}(10, 36)\\). Value \\(v\\) for companies (N) not issuing a dividend = \\(v_N \\sim \\mathcal{N}(0, 36)\\) and \\(p(D) = 0.8\\). We want to find \\(p(D|v)\\) and we can calculate \\(p(v|D)\\) from the Gaussian density function. Note that since \\(e^2\\) is equal between both classes, the term \\(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\) cancels. \\[\\begin{align} p(D|v) &amp;= \\frac{p(v|D) p(D)}{p(v|D)p(D) + p(v|N)p(N)} \\\\ &amp;= \\frac{\\pi_D \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-(x-\\mu_D)^2/2\\sigma^2}} {\\pi_D \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-(x-\\mu_D)^2/2\\sigma^2} + \\pi_N \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-(x-\\mu_N)^2/2\\sigma^2}} \\\\ &amp;= \\frac{\\pi_D e^{-(x-\\mu_D)^2/2\\sigma^2}} {\\pi_D e^{-(x-\\mu_D)^2/2\\sigma^2} + \\pi_N e^{-(x-\\mu_N)^2/2\\sigma^2}} \\\\ &amp;= \\frac{0.8 \\times e^{-(4-10)^2/(2 \\times 36)}} {0.8 \\times e^{-(4-10)^2/(2 \\times 36)} + 0.2 \\times e^{-(4-0)^2/(2 \\times 36)}} \\\\ &amp;= \\frac{0.8 \\times e^{-1/2}}{0.8 \\times e^{-1/2} + 0.2 \\times e^{-2/9}} \\end{align}\\] exp(-0.5) * 0.8 / (exp(-0.5) * 0.8 + exp(-2/9) * 0.2) ## [1] 0.7518525 4.1.8 Question 8 Suppose that we take a data set, divide it into equally-sized training and test sets, and then try out two different classification procedures. First we use logistic regression and get an error rate of 20% on the training data and 30% on the test data. Next we use 1-nearest neighbors (i.e. \\(K = 1\\)) and get an average error rate (averaged over both test and training data sets) of 18%. Based on these results, which method should we prefer to use for classification of new observations? Why? For \\(K = 1\\), performance on the training set is perfect and the error rate is zero, implying a test error rate of 36%. Logistic regression outperforms 1-nearest neighbor on the test set and therefore should be preferred. 4.1.9 Question 9 This problem has to do with odds. On average, what fraction of people with an odds of 0.37 of defaulting on their credit card payment will in fact default? Odds is defined as \\(p/(1-p)\\). \\[0.37 = \\frac{p(x)}{1 - p(x)}\\] therefore, \\[p(x) = \\frac{0.37}{1 + 0.37} = 0.27\\] Suppose that an individual has a 16% chance of defaulting on her credit card payment. What are the odds that she will default? \\[0.16 / (1 - 0.16) = 0.19\\] 4.1.10 Question 10 ToDo Equation 4.32 derived an expression for \\(\\log(\\frac{Pr(Y=k|X=x)}{Pr(Y=K|X=x)})\\) in the setting where \\(p &gt; 1\\), so that the mean for the \\(k\\)th class, \\(\\mu_k\\), is a \\(p\\)-dimensional vector, and the shared covariance \\(\\Sigma\\) is a \\(p \\times p\\) matrix. However, in the setting with \\(p = 1\\), (4.32) takes a simpler form, since the means \\(\\mu_1, ..., \\mu_k\\) and the variance \\(\\sigma^2\\) are scalars. In this simpler setting, repeat the calculation in (4.32), and provide expressions for \\(a_k\\) and \\(b_{kj}\\) in terms of \\(\\pi_k, \\pi_K, \\mu_k, \\mu_K,\\) and \\(\\sigma^2\\). 4.1.11 Question 11 ToDo Work out the detailed forms of \\(a_k\\), \\(b_{kj}\\), and \\(b_{kjl}\\) in (4.33). Your answer should involve \\(\\pi_k\\), \\(\\pi_K\\), \\(\\mu_k\\), \\(\\mu_K\\), \\(\\Sigma_k\\), and \\(\\Sigma_K\\). 4.1.12 Question 12 ToDo Suppose that you wish to classify an observation \\(X \\in \\mathbb{R}\\) into apples and oranges. You fit a logistic regression model and find that \\[ \\hat{Pr}(Y=orange|X=x) = \\frac{\\exp(\\hat\\beta_0 + \\hat\\beta_1x)}{1 + \\exp(\\hat\\beta_0 + \\hat\\beta_1x)} \\] Your friend fits a logistic regression model to the same data using the softmax formulation in (4.13), and finds that \\[ \\hat{Pr}(Y=orange|X=x) = \\frac{\\exp(\\hat\\alpha_{orange0} + \\hat\\alpha_{orange1}x)} {\\exp(\\hat\\alpha_{orange0} + \\hat\\alpha_{orange1}x) + \\exp(\\hat\\alpha_{apple0} + \\hat\\alpha_{apple1}x)} \\] What is the log odds of orange versus apple in your model? What is the log odds of orange versus apple in your friend’s model? Suppose that in your model, \\(\\hat\\beta_0 = 2\\) and \\(\\hat\\beta = −1\\). What are the coefficient estimates in your friend’s model? Be as specific as possible. Now suppose that you and your friend fit the same two models on a different data set. This time, your friend gets the coefficient estimates \\(\\hat\\alpha_{orange0} = 1.2\\), \\(\\hat\\alpha_{orange1} = −2\\), \\(\\hat\\alpha_{apple0} = 3\\), \\(\\hat\\alpha_{apple1} = 0.6\\). What are the coefficient estimates in your model? Finally, suppose you apply both models from (d) to a data set with 2,000 test observations. What fraction of the time do you expect the predicted class labels from your model to agree with those from your friend’s model? Explain your answer. 4.2 Applied 4.2.1 Question 13 This question should be answered using the Weekly data set, which is part of the ISLR2 package. This data is similar in nature to the Smarket data from this chapter’s lab, except that it contains 1,089 weekly returns for 21 years, from the beginning of 1990 to the end of 2010. Produce some numerical and graphical summaries of the Weekly data. Do there appear to be any patterns? library(MASS) library(class) library(tidyverse) library(corrplot) library(ISLR2) summary(Weekly) ## Year Lag1 Lag2 Lag3 ## Min. :1990 Min. :-18.1950 Min. :-18.1950 Min. :-18.1950 ## 1st Qu.:1995 1st Qu.: -1.1540 1st Qu.: -1.1540 1st Qu.: -1.1580 ## Median :2000 Median : 0.2410 Median : 0.2410 Median : 0.2410 ## Mean :2000 Mean : 0.1506 Mean : 0.1511 Mean : 0.1472 ## 3rd Qu.:2005 3rd Qu.: 1.4050 3rd Qu.: 1.4090 3rd Qu.: 1.4090 ## Max. :2010 Max. : 12.0260 Max. : 12.0260 Max. : 12.0260 ## Lag4 Lag5 Volume Today ## Min. :-18.1950 Min. :-18.1950 Min. :0.08747 Min. :-18.1950 ## 1st Qu.: -1.1580 1st Qu.: -1.1660 1st Qu.:0.33202 1st Qu.: -1.1540 ## Median : 0.2380 Median : 0.2340 Median :1.00268 Median : 0.2410 ## Mean : 0.1458 Mean : 0.1399 Mean :1.57462 Mean : 0.1499 ## 3rd Qu.: 1.4090 3rd Qu.: 1.4050 3rd Qu.:2.05373 3rd Qu.: 1.4050 ## Max. : 12.0260 Max. : 12.0260 Max. :9.32821 Max. : 12.0260 ## Direction ## Down:484 ## Up :605 ## ## ## ## corrplot(cor(Weekly[, -9]), type = &quot;lower&quot;, diag = FALSE, method = &quot;ellipse&quot;) Volume is strongly positively correlated with Year. Other correlations are week, but Lag1 is negatively correlated with Lag2 but positively correlated with Lag3. Use the full data set to perform a logistic regression with Direction as the response and the five lag variables plus Volume as predictors. Use the summary function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones? fit &lt;- glm( Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly, family = binomial ) summary(fit) ## ## Call: ## glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + ## Volume, family = binomial, data = Weekly) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.6949 -1.2565 0.9913 1.0849 1.4579 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.26686 0.08593 3.106 0.0019 ** ## Lag1 -0.04127 0.02641 -1.563 0.1181 ## Lag2 0.05844 0.02686 2.175 0.0296 * ## Lag3 -0.01606 0.02666 -0.602 0.5469 ## Lag4 -0.02779 0.02646 -1.050 0.2937 ## Lag5 -0.01447 0.02638 -0.549 0.5833 ## Volume -0.02274 0.03690 -0.616 0.5377 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1496.2 on 1088 degrees of freedom ## Residual deviance: 1486.4 on 1082 degrees of freedom ## AIC: 1500.4 ## ## Number of Fisher Scoring iterations: 4 Lag2 is significant. Compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression. contrasts(Weekly$Direction) ## Up ## Down 0 ## Up 1 pred &lt;- predict(fit, type = &quot;response&quot;) &gt; 0.5 (t &lt;- table(ifelse(pred, &quot;Up (pred)&quot;, &quot;Down (pred)&quot;), Weekly$Direction)) ## ## Down Up ## Down (pred) 54 48 ## Up (pred) 430 557 sum(diag(t)) / sum(t) ## [1] 0.5610652 The overall fraction of correct predictions is 0.56. Although logistic regression correctly predicts upwards movements well, it incorrectly predicts most downwards movements as up. Now fit the logistic regression model using a training data period from 1990 to 2008, with Lag2 as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010). train &lt;- Weekly$Year &lt; 2009 fit &lt;- glm(Direction ~ Lag2, data = Weekly[train, ], family = binomial) pred &lt;- predict(fit, Weekly[!train, ], type = &quot;response&quot;) &gt; 0.5 (t &lt;- table(ifelse(pred, &quot;Up (pred)&quot;, &quot;Down (pred)&quot;), Weekly[!train, ]$Direction)) ## ## Down Up ## Down (pred) 9 5 ## Up (pred) 34 56 sum(diag(t)) / sum(t) ## [1] 0.625 Repeat (d) using LDA. fit &lt;- lda(Direction ~ Lag2, data = Weekly[train, ]) pred &lt;- predict(fit, Weekly[!train, ], type = &quot;response&quot;)$class (t &lt;- table(pred, Weekly[!train, ]$Direction)) ## ## pred Down Up ## Down 9 5 ## Up 34 56 sum(diag(t)) / sum(t) ## [1] 0.625 Repeat (d) using QDA. fit &lt;- qda(Direction ~ Lag2, data = Weekly[train, ]) pred &lt;- predict(fit, Weekly[!train, ], type = &quot;response&quot;)$class (t &lt;- table(pred, Weekly[!train, ]$Direction)) ## ## pred Down Up ## Down 0 0 ## Up 43 61 sum(diag(t)) / sum(t) ## [1] 0.5865385 Repeat (d) using KNN with \\(K = 1\\). fit &lt;- knn( Weekly[train, &quot;Lag2&quot;, drop = FALSE], Weekly[!train, &quot;Lag2&quot;, drop = FALSE], Weekly$Direction[train] ) (t &lt;- table(fit, Weekly[!train, ]$Direction)) ## ## fit Down Up ## Down 21 30 ## Up 22 31 sum(diag(t)) / sum(t) ## [1] 0.5 Repeat (d) using naive Bayes. ToDo Which of these methods appears to provide the best results on this data? ToDo Logistic regression and LDA are the best performing. Experiment with different combinations of predictors, including possible transformations and interactions, for each of the methods. Report the variables, method, and associated confusion matrix that appears to provide the best results on the held out data. Note that you should also experiment with values for \\(K\\) in the KNN classifier. ToDo fit &lt;- glm(Direction ~ Lag1, data = Weekly[train, ], family = binomial) pred &lt;- predict(fit, Weekly[!train, ], type = &quot;response&quot;) &gt; 0.5 mean(ifelse(pred, &quot;Up&quot;, &quot;Down&quot;) == Weekly[!train, ]$Direction) ## [1] 0.5673077 fit &lt;- glm(Direction ~ Lag3, data = Weekly[train, ], family = binomial) pred &lt;- predict(fit, Weekly[!train, ], type = &quot;response&quot;) &gt; 0.5 mean(ifelse(pred, &quot;Up&quot;, &quot;Down&quot;) == Weekly[!train, ]$Direction) ## [1] 0.5865385 fit &lt;- glm(Direction ~Lag4, data = Weekly[train, ], family = binomial) pred &lt;- predict(fit, Weekly[!train, ], type = &quot;response&quot;) &gt; 0.5 mean(ifelse(pred, &quot;Up&quot;, &quot;Down&quot;) == Weekly[!train, ]$Direction) ## [1] 0.5865385 fit &lt;- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4, data = Weekly[train, ], family = binomial) pred &lt;- predict(fit, Weekly[!train, ], type = &quot;response&quot;) &gt; 0.5 mean(ifelse(pred, &quot;Up&quot;, &quot;Down&quot;) == Weekly[!train, ]$Direction) ## [1] 0.5865385 fit &lt;- glm(Direction ~ Lag1 * Lag2 * Lag3 * Lag4, data = Weekly[train, ], family = binomial) pred &lt;- predict(fit, Weekly[!train, ], type = &quot;response&quot;) &gt; 0.5 mean(ifelse(pred, &quot;Up&quot;, &quot;Down&quot;) == Weekly[!train, ]$Direction) ## [1] 0.5961538 fit &lt;- lda(Direction ~ Lag1 + Lag2 + Lag3 + Lag4,data = Weekly[train, ]) pred &lt;- predict(fit, Weekly[!train, ], type = &quot;response&quot;)$class mean(pred == Weekly[!train, ]$Direction) ## [1] 0.5769231 fit &lt;- qda(Direction ~ Lag1 + Lag2 + Lag3 + Lag4, data = Weekly[train, ]) pred &lt;- predict(fit, Weekly[!train, ], type = &quot;response&quot;)$class mean(pred == Weekly[!train, ]$Direction) ## [1] 0.5192308 set.seed(1) res &lt;- sapply(1:30, function(k) { fit &lt;- knn( Weekly[train, 2:4, drop = FALSE], Weekly[!train, 2:4, drop = FALSE], Weekly$Direction[train], k = k ) mean(fit == Weekly[!train, ]$Direction) }) plot(1:30, res, type = &quot;o&quot;, xlab = &quot;k&quot;, ylab = &quot;Fraction correct&quot;) (k &lt;- which.max(res)) ## [1] 26 fit &lt;- knn( Weekly[train, 2:4, drop = FALSE], Weekly[!train, 2:4, drop = FALSE], Weekly$Direction[train], k = k ) table(fit, Weekly[!train, ]$Direction) ## ## fit Down Up ## Down 23 18 ## Up 20 43 mean(fit == Weekly[!train, ]$Direction) ## [1] 0.6346154 KNN using the first 3 Lag variables performs marginally better than logistic regression with Lag2 if we tune \\(k\\). 4.2.2 Question 14 In this problem, you will develop a model to predict whether a given car gets high or low gas mileage based on the Auto data set. Create a binary variable, mpg01, that contains a 1 if mpg contains a value above its median, and a 0 if mpg contains a value below its median. You can compute the median using the median() function. Note you may find it helpful to use the data.frame() function to create a single data set containing both mpg01 and the other Auto variables. x &lt;- cbind(Auto[, -1], data.frame(&quot;mpg01&quot; = Auto$mpg &gt; median(Auto$mpg))) Explore the data graphically in order to investigate the association between mpg01 and the other features. Which of the other features seem most likely to be useful in predicting mpg01? Scatterplots and boxplots may be useful tools to answer this question. Describe your findings. par(mfrow = c(2, 4)) for (i in 1:7) hist(x[, i], breaks = 20, main = colnames(x)[i]) par(mfrow = c(2, 4)) for (i in 1:7) boxplot(x[, i] ~ x$mpg01, main = colnames(x)[i]) pairs(x[, 1:7]) Most variables show an association with mpg01 category, and several variables are colinear. Split the data into a training set and a test set. set.seed(1) train &lt;- sample(seq_len(nrow(x)), nrow(x) * 2/3) Perform LDA on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained? sort(sapply(1:7, function(i) { setNames(abs(t.test(x[, i] ~ x$mpg01)$statistic), colnames(x)[i]) })) ## acceleration year origin horsepower displacement weight ## 7.302430 9.403221 11.824099 17.681939 22.632004 22.932777 ## cylinders ## 23.035328 fit &lt;- lda(mpg01 ~ cylinders + weight + displacement, data = x[train, ]) pred &lt;- predict(fit, x[-train, ], type = &quot;response&quot;)$class mean(pred != x[-train, ]$mpg01) ## [1] 0.1068702 Perform QDA on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained? fit &lt;- qda(mpg01 ~ cylinders + weight + displacement, data = x[train, ]) pred &lt;- predict(fit, x[-train, ], type = &quot;response&quot;)$class mean(pred != x[-train, ]$mpg01) ## [1] 0.09923664 Perform logistic regression on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained? fit &lt;- glm(mpg01 ~ cylinders + weight + displacement, data = x[train, ], family = binomial) pred &lt;- predict(fit, x[-train, ], type = &quot;response&quot;) &gt; 0.5 mean(pred != x[-train, ]$mpg01) ## [1] 0.1145038 Perform naive Bayes on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained? ToDo Perform KNN on the training data, with several values of \\(K\\), in order to predict mpg01. Use only the variables that seemed most associated with mpg01 in (b). What test errors do you obtain? Which value of \\(K\\) seems to perform the best on this data set? res &lt;- sapply(1:50, function(k) { fit &lt;- knn(x[train, c(1, 4, 2)], x[-train, c(1, 4, 2)], x$mpg01[train], k = k) mean(fit != x[-train, ]$mpg01) }) names(res) &lt;- 1:50 plot(res, type = &quot;o&quot;) res[which.min(res)] ## 3 ## 0.1068702 For the models tested here, \\(k = 32\\) appears to perform best. QDA has a lower error rate overall, performing slightly better than LDA. 4.2.3 Question 15 This problem involves writing functions. Write a function, Power(), that prints out the result of raising 2 to the 3rd power. In other words, your function should compute \\(2^3\\) and print out the results. Hint: Recall that x^a raises x to the power a. Use the print() function to output the result. Power &lt;- function() print(2^3) Create a new function, Power2(), that allows you to pass any two numbers, x and a, and prints out the value of x^a. You can do this by beginning your function with the line &gt; Power2=function(x,a) { You should be able to call your function by entering, for instance, &gt; Power2(3, 8) on the command line. This should output the value of \\(3^8\\), namely, 6,561. Power2 &lt;- function(x, a) print(x^a) Using the Power2() function that you just wrote, compute \\(10^3\\), \\(8^{17}\\), and \\(131^3\\). c(Power2(10, 3), Power2(8, 17), Power2(131, 3)) ## [1] 1000 ## [1] 2.2518e+15 ## [1] 2248091 ## [1] 1.000000e+03 2.251800e+15 2.248091e+06 Now create a new function, Power3(), that actually returns the result x^a as an R object, rather than simply printing it to the screen. That is, if you store the value x^a in an object called result within your function, then you can simply return() this result, using the following line: &gt; return(result) The line above should be the last line in your function, before the } symbol. Power3 &lt;- function(x, a) { result &lt;- x^a return(result) } Now using the Power3() function, create a plot of \\(f(x) = x^2\\). The \\(x\\)-axis should display a range of integers from 1 to 10, and the \\(y\\)-axis should display \\(x^2\\). Label the axes appropriately, and use an appropriate title for the figure. Consider displaying either the \\(x\\)-axis, the \\(y\\)-axis, or both on the log-scale. You can do this by using log = \"x\", log = \"y\", or log = \"xy\" as arguments to the plot() function. plot(1:10, Power3(1:10, 2), xlab = &quot;x&quot;, ylab = expression(paste(&quot;x&quot;^&quot;2&quot;)), log = &quot;y&quot; ) Create a function, PlotPower(), that allows you to create a plot of x against x^a for a fixed a and for a range of values of x. For instance, if you call &gt; PlotPower(1:10, 3) then a plot should be created with an \\(x\\)-axis taking on values \\(1,2,...,10\\), and a \\(y\\)-axis taking on values \\(1^3,2^3,...,10^3\\). PlotPower &lt;- function(x, a, log = &quot;y&quot;) { plot(x, Power3(x, a), xlab = &quot;x&quot;, ylab = substitute(&quot;x&quot;^a, list(a = a)), log = log ) } PlotPower(1:10, 3) 4.2.4 Question 13 Using the Boston data set, fit classification models in order to predict whether a given census tract has a crime rate above or below the median. Explore logistic regression, LDA, naive Bayes and KNN models using various sub-sets of the predictors. Describe your findings. Hint: You will have to create the response variable yourself, using the variables that are contained in the Boston data set. ToDo x &lt;- cbind( Boston[, -1], data.frame(&quot;highcrim&quot; = Boston$crim &gt; median(Boston$crim)) ) set.seed(1) train &lt;- sample(seq_len(nrow(x)), nrow(x) * 2/3) Fit lda, logistic regression and KNN models (with k = 1..50) for a set of specific predictors and return the error rate. f &lt;- function(cols, k_vals = 1:50) { dat_train &lt;- x[train, cols, drop = FALSE] dat_test &lt;- x[-train, cols, drop = FALSE] fit &lt;- lda(x$highcrim[train] ~ ., data = dat_train) pred &lt;- predict(fit, dat_test, type = &quot;response&quot;)$class lda_err &lt;- mean(pred != x$highcrim[-train]) fit &lt;- glm(x$highcrim[train] ~ ., data = dat_train, family = binomial) pred &lt;- predict(fit, dat_test, type = &quot;response&quot;) &gt; 0.5 logreg_err &lt;- mean(pred != x$highcrim[-train]) res &lt;- sapply(k_vals, function(k) { fit &lt;- knn(dat_train, dat_test, x$highcrim[train], k = k) mean(fit != x$highcrim[-train]) }) knn_err &lt;- min(res) c(&quot;LDA&quot; = lda_err, &quot;LR&quot; = logreg_err, &quot;KNN&quot; = knn_err) } n &lt;- sort(sapply(1:13, function(i) { setNames(abs(wilcox.test(as.numeric(x[, i]) ~ x$highcrim)$p.value), colnames(x)[i]) })) n ## highcrim nox dis age indus ## 7.825553e-112 2.412920e-64 1.056245e-48 2.938048e-45 3.100164e-44 ## tax rad zn lstat medv ## 1.902875e-41 5.061325e-38 3.505222e-27 4.460639e-25 4.896101e-19 ## ptratio rm chas ## 2.666494e-16 5.164707e-05 1.153633e-01 Variables nox (nitrogen oxides concentration) followed by dis (distance to employment center) appear to be most associated with high crime. res &lt;- sapply(1:13, function(max) f(1:max)) (res &lt;- as_tibble(t(res))) ## # A tibble: 13 × 3 ## LDA LR KNN ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.296 0.296 0.296 ## 2 0.249 0.225 0.0296 ## 3 0.243 0.225 0.0296 ## 4 0.178 0.118 0.0296 ## 5 0.172 0.130 0.0533 ## 6 0.166 0.142 0.154 ## 7 0.160 0.142 0.130 ## 8 0.148 0.130 0.107 ## 9 0.154 0.130 0.0769 ## 10 0.154 0.101 0.0710 ## 11 0.154 0.130 0.0769 ## 12 0.166 0.118 0.0769 ## 13 0.166 0.118 0.0769 res$n_var &lt;- 1:13 pivot_longer(res, cols = !n_var) |&gt; ggplot(aes(n_var, value, col = name)) + geom_line() + xlab(&quot;Number of predictors&quot;) + ylab(&quot;Error rate&quot;) KNN appears to perform better (if we tune \\(k\\)) for all numbers of predictors. fit &lt;- knn( x[train, &quot;nox&quot;, drop = FALSE], x[-train, &quot;nox&quot;, drop = FALSE], x$highcrim[train], k = 1 ) table(fit, x[-train, ]$highcrim) ## ## fit FALSE TRUE ## FALSE 78 2 ## TRUE 3 86 mean(fit != x[-train, ]$highcrim) * 100 ## [1] 2.95858 Surprisingly, the best model (with an error rate of ~5%) uses \\(k = 1\\) and assigns crime rate categories based on the town with the single most similar nitrogen oxide concentration. This might be, for example, because nearby towns have similar crime rates, and we can obtain good predictions by predicting crime rate based on a nearby town. But what if we only consider \\(k = 20\\) and up to 8 predictors. res &lt;- sapply(1:8, function(max) f(1:max, k_vals = 20)) (res &lt;- as_tibble(t(res))) ## # A tibble: 8 × 3 ## LDA LR KNN ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.296 0.296 0.296 ## 2 0.249 0.225 0.172 ## 3 0.243 0.225 0.166 ## 4 0.178 0.118 0.172 ## 5 0.172 0.130 0.154 ## 6 0.166 0.142 0.213 ## 7 0.160 0.142 0.201 ## 8 0.148 0.130 0.195 res$n_var &lt;- 1:8 pivot_longer(res, cols = !n_var) |&gt; ggplot(aes(n_var, value, col = name)) + geom_line() + xlab(&quot;Number of predictors&quot;) + ylab(&quot;Error rate&quot;) We can see that there best model now is logistic regression with 6 predictors which has ~10% error rate. (vars &lt;- names(n)[1:6]) ## [1] &quot;highcrim&quot; &quot;nox&quot; &quot;dis&quot; &quot;age&quot; &quot;indus&quot; &quot;tax&quot; dat_train &lt;- x[train, vars] dat_test &lt;- x[-train, vars] (fit &lt;- glm(x$highcrim[train] ~ ., data = dat_train, family = binomial)) ## ## Call: glm(formula = x$highcrim[train] ~ ., family = binomial, data = dat_train) ## ## Coefficients: ## (Intercept) nox dis age indus tax ## -21.205733 36.357909 0.270415 0.006280 -0.102454 0.003696 ## ## Degrees of Freedom: 336 Total (i.e. Null); 331 Residual ## Null Deviance: 467 ## Residual Deviance: 206.5 AIC: 218.5 pred &lt;- predict(fit, dat_test, type = &quot;response&quot;) &gt; 0.5 table(pred, x[-train, ]$highcrim) ## ## pred FALSE TRUE ## FALSE 65 10 ## TRUE 16 78 mean(pred != x$highcrim[-train]) * 100 ## [1] 15.38462 summary(fit) ## ## Call: ## glm(formula = x$highcrim[train] ~ ., family = binomial, data = dat_train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.0291 -0.4497 -0.1615 0.2717 2.3764 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -21.205733 3.575588 -5.931 3.02e-09 *** ## nox 36.357909 6.546083 5.554 2.79e-08 *** ## dis 0.270415 0.170125 1.590 0.1119 ## age 0.006280 0.009748 0.644 0.5194 ## indus -0.102454 0.044403 -2.307 0.0210 * ## tax 0.003696 0.001594 2.319 0.0204 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 467.04 on 336 degrees of freedom ## Residual deviance: 206.47 on 331 degrees of freedom ## AIC: 218.47 ## ## Number of Fisher Scoring iterations: 7 "],["resampling-methods.html", "5 Resampling Methods 5.1 Conceptual 5.2 Applied", " 5 Resampling Methods 5.1 Conceptual 5.1.1 Question 1 Using basic statistical properties of the variance, as well as single- variable calculus, derive (5.6). In other words, prove that \\(\\alpha\\) given by (5.6) does indeed minimize \\(Var(\\alpha X + (1 − \\alpha)Y)\\). Equation 5.6 is: \\[ \\alpha = \\frac{\\sigma^2_Y - \\sigma_{XY}}{\\sigma^2_X + \\sigma^2_Y - 2\\sigma_{XY}} \\] Remember that: \\[ Var(aX) = a^2Var(X), \\\\ \\mathrm{Var}(X + Y) = \\mathrm{Var}(X) + \\mathrm{Var}(Y) + \\mathrm{Cov}(X,Y), \\\\ \\mathrm{Cov}(aX, bY) = ab\\mathrm{Cov}(X, Y) \\] If we define \\(\\sigma^2_X = \\mathrm{Var}(X)\\), \\(\\sigma^2_Y = \\mathrm{Var}(Y)\\) and \\(\\sigma_{XY} = \\mathrm{Cov}(X, Y)\\) \\[\\begin{align} Var(\\alpha X + (1 - \\alpha)Y) &amp;= \\alpha^2\\sigma^2_X + (1-\\alpha)^2\\sigma^2_Y + 2\\alpha(1 - \\alpha)\\sigma_{XY} \\\\ &amp;= \\alpha^2\\sigma^2_X + \\sigma^2_Y - 2\\alpha\\sigma^2_Y + \\alpha^2\\sigma^2_Y + 2\\alpha\\sigma_{XY} - 2\\alpha^2\\sigma_{XY} \\end{align}\\] Now we want to find when the rate of change of this function is 0 with respect to \\(\\alpha\\), so we compute the partial derivative, set to 0 and solve. \\[ \\frac{\\partial}{\\partial{\\alpha}} = 2\\alpha\\sigma^2_X - 2\\sigma^2_Y + 2\\alpha\\sigma^2_Y + 2\\sigma_{XY} - 4\\alpha\\sigma_{XY} = 0 \\] Moving \\(\\alpha\\) terms to the same side: \\[ \\alpha\\sigma^2_X + \\alpha\\sigma^2_Y - 2\\alpha\\sigma_{XY} = \\sigma^2_Y - \\sigma_{XY} \\] \\[ \\alpha = \\frac{\\sigma^2_Y - \\sigma_{XY}}{\\sigma^2_X + \\sigma^2_Y - 2\\sigma_{XY}} \\] We should also show that this is a minimum, so that the second partial derivative wrt \\(\\alpha\\) is \\(&gt;= 0\\). \\[\\begin{align} \\frac{\\partial^2}{\\partial{\\alpha^2}} &amp;= 2\\sigma^2_X + 2\\sigma^2_Y - 4\\sigma_{XY} \\\\ &amp;= 2(\\sigma^2_X + \\sigma^2_Y - 2\\sigma_{XY}) \\\\ &amp;= 2\\mathrm{Var}(X - Y) \\end{align}\\] Since variance is positive, then this must be positive. 5.1.2 Question 2 We will now derive the probability that a given observation is part of a bootstrap sample. Suppose that we obtain a bootstrap sample from a set of n observations. What is the probability that the first bootstrap observation is not the \\(j\\)th observation from the original sample? Justify your answer. This is 1 - probability that it is the \\(j\\)th = \\(1 - 1/n\\). What is the probability that the second bootstrap observation is not the \\(j\\)th observation from the original sample? Since each bootstrap observation is a random sample, this probability is the same (\\(1 - 1/n\\)). Argue that the probability that the \\(j\\)th observation is not in the bootstrap sample is \\((1 − 1/n)^n\\). For the \\(j\\)th observation to not be in the sample, it would have to not be picked for each of \\(n\\) positions, so not picked for \\(1, 2, ..., n\\), thus the probability is \\((1 - 1/n)^n\\) When \\(n = 5\\), what is the probability that the \\(j\\)th observation is in the bootstrap sample? n &lt;- 5 1 - (1 - 1/n)^n ## [1] 0.67232 \\(p = 0.67\\) When \\(n = 100\\), what is the probability that the \\(j\\)th observation is in the bootstrap sample? n &lt;- 100 1 - (1 - 1/n)^n ## [1] 0.6339677 \\(p = 0.64\\) When \\(n = 10,000\\), what is the probability that the \\(j\\)th observation is in the bootstrap sample? n &lt;- 100000 1 - (1 - 1/n)^n ## [1] 0.6321224 \\(p = 0.63\\) Create a plot that displays, for each integer value of \\(n\\) from 1 to 100,000, the probability that the \\(j\\)th observation is in the bootstrap sample. Comment on what you observe. x &lt;- sapply(1:100000, function(n) 1 - (1 - 1/n)^n) plot(x, log = &quot;x&quot;, type = &quot;o&quot;) The probability rapidly approaches 0.63 with increasing \\(n\\). Note that \\[e^x = \\lim_{x \\to \\inf} \\left(1 + \\frac{x}{n}\\right)^n,\\] so with \\(x = -1\\), we can see that our limit is \\(1 - e^{-1} = 1 - 1/e\\). We will now investigate numerically the probability that a bootstrap sample of size \\(n = 100\\) contains the \\(j\\)th observation. Here \\(j = 4\\). We repeatedly create bootstrap samples, and each time we record whether or not the fourth observation is contained in the bootstrap sample. &gt; store &lt;- rep (NA, 10000) &gt; for (i in 1:10000) { store[i] &lt;- sum(sample(1:100, rep = TRUE) == 4) &gt; 0 } &gt; mean(store) Comment on the results obtained. store &lt;- replicate(10000, sum(sample(1:100, replace = TRUE) == 4) &gt; 0) mean(store) ## [1] 0.6291 The probability of including \\(4\\) when resampling numbers \\(1...100\\) is close to \\(1 - (1 - 1/100)^{100}\\). 5.1.3 Question 3 We now review \\(k\\)-fold cross-validation. Explain how \\(k\\)-fold cross-validation is implemented. We divided our data into (approximately equal) \\(k\\) subsets, and then generate predictions for each \\(k\\)th set, training on the exclusive \\(k\\) sets combined. What are the advantages and disadvantages of \\(k\\)-fold cross-validation relative to: The validation set approach? LOOCV? When using a validation set, we can only train on a small portion of the data as we must reserve the rest for validation. As a result it can overestimate the test error rate (assuming we then train using the complete data for future prediction). It is also sensitive to which observations are including in train vs. test. It is, however, low cost in terms of processing time (as we only have to fit one model). When using LOOCV, we can train on \\(n-1\\) observations, however, the trained models we generate each differ only by the inclusion (and exclusion) of a single observation. As a result, LOOCV can have high variance (the models fit will be similar, and might be quite different to what we would obtain with a different data set). LOOCV is also costly in terms of processing time. 5.1.4 Question 4 Suppose that we use some statistical learning method to make a prediction for the response \\(Y\\) for a particular value of the predictor \\(X\\). Carefully describe how we might estimate the standard deviation of our prediction. We could address this with bootstrapping. Our procedure would be to (jointly) resample \\(Y\\) and \\(X\\) variables and fit our model many times. For each model we could obtain a summary of our prediction and calculate the standard deviation over bootstrapped samples. 5.2 Applied 5.2.1 Question 5 In Chapter 4, we used logistic regression to predict the probability of default using income and balance on the Default data set. We will now estimate the test error of this logistic regression model using the validation set approach. Do not forget to set a random seed before beginning your analysis. Fit a logistic regression model that uses income and balance to predict default. library(ISLR2) set.seed(42) fit &lt;- glm(default ~ income + balance, data = Default, family = &quot;binomial&quot;) Using the validation set approach, estimate the test error of this model. In order to do this, you must perform the following steps: Split the sample set into a training set and a validation set. Fit a multiple logistic regression model using only the training observations. Obtain a prediction of default status for each individual in the validation set by computing the posterior probability of default for that individual, and classifying the individual to the default category if the posterior probability is greater than 0.5. Compute the validation set error, which is the fraction of the observations in the validation set that are misclassified. train &lt;- sample(nrow(Default), nrow(Default) / 2) fit &lt;- glm(default ~ income + balance, data = Default, family = &quot;binomial&quot;, subset = train) pred &lt;- ifelse(predict(fit, newdata = Default[-train, ], type = &quot;response&quot;) &gt; 0.5, &quot;Yes&quot;, &quot;No&quot;) table(pred, Default$default[-train]) ## ## pred No Yes ## No 4817 110 ## Yes 20 53 mean(pred != Default$default[-train]) ## [1] 0.026 Repeat the process in (b) three times, using three different splits of the observations into a training set and a validation set. Comment on the results obtained. replicate(3, { train &lt;- sample(nrow(Default), nrow(Default) / 2) fit &lt;- glm(default ~ income + balance, data = Default, family = &quot;binomial&quot;, subset = train) pred &lt;- ifelse(predict(fit, newdata = Default[-train, ], type = &quot;response&quot;) &gt; 0.5, &quot;Yes&quot;, &quot;No&quot;) mean(pred != Default$default[-train]) }) ## [1] 0.0260 0.0294 0.0258 The results obtained are variable and depend on the samples allocated to training vs. test. Now consider a logistic regression model that predicts the probability of default using income, balance, and a dummy variable for student. Estimate the test error for this model using the validation set approach. Comment on whether or not including a dummy variable for student leads to a reduction in the test error rate. replicate(3, { train &lt;- sample(nrow(Default), nrow(Default) / 2) fit &lt;- glm(default ~ income + balance + student, data = Default, family = &quot;binomial&quot;, subset = train) pred &lt;- ifelse(predict(fit, newdata = Default[-train, ], type = &quot;response&quot;) &gt; 0.5, &quot;Yes&quot;, &quot;No&quot;) mean(pred != Default$default[-train]) }) ## [1] 0.0278 0.0256 0.0250 Including student does not seem to make a substantial improvement to the test error. 5.2.2 Question 6 We continue to consider the use of a logistic regression model to predict the probability of default using income and balance on the Default data set. In particular, we will now compute estimates for the standard errors of the income and balance logistic regression coefficients in two different ways: (1) using the bootstrap, and (2) using the standard formula for computing the standard errors in the glm() function. Do not forget to set a random seed before beginning your analysis. Using the summary() and glm() functions, determine the estimated standard errors for the coefficients associated with income and balance in a multiple logistic regression model that uses both predictors. fit &lt;- glm(default ~ income + balance, data = Default, family = &quot;binomial&quot;) summary(fit) ## ## Call: ## glm(formula = default ~ income + balance, family = &quot;binomial&quot;, ## data = Default) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.4725 -0.1444 -0.0574 -0.0211 3.7245 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.154e+01 4.348e-01 -26.545 &lt; 2e-16 *** ## income 2.081e-05 4.985e-06 4.174 2.99e-05 *** ## balance 5.647e-03 2.274e-04 24.836 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2920.6 on 9999 degrees of freedom ## Residual deviance: 1579.0 on 9997 degrees of freedom ## AIC: 1585 ## ## Number of Fisher Scoring iterations: 8 The standard errors obtained by bootstrapping are \\(\\beta_1\\) = 5.0e-6 and \\(\\beta_2\\) = 2.3e-4. Write a function, boot.fn(), that takes as input the Default data set as well as an index of the observations, and that outputs the coefficient estimates for income and balance in the multiple logistic regression model. boot.fn &lt;- function(x, i) { fit &lt;- glm(default ~ income + balance, data = Default[i, ], family = &quot;binomial&quot;) coef(fit)[-1] } Use the boot() function together with your boot.fn() function to estimate the standard errors of the logistic regression coefficients for income and balance. library(boot) set.seed(42) boot(Default, boot.fn, R = 1000) ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = Default, statistic = boot.fn, R = 1000) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 2.080898e-05 2.737444e-08 5.073444e-06 ## t2* 5.647103e-03 1.176249e-05 2.299133e-04 Comment on the estimated standard errors obtained using the glm() function and using your bootstrap function. The standard errors obtained by bootstrapping are similar to those estimated by glm. 5.2.3 Question 7 In Sections 5.3.2 and 5.3.3, we saw that the cv.glm() function can be used in order to compute the LOOCV test error estimate. Alternatively, one could compute those quantities using just the glm() and predict.glm() functions, and a for loop. You will now take this approach in order to compute the LOOCV error for a simple logistic regression model on the Weekly data set. Recall that in the context of classification problems, the LOOCV error is given in (5.4). Fit a logistic regression model that predicts Direction using Lag1 and Lag2. fit &lt;- glm(Direction ~ Lag1 + Lag2, data = Weekly, family = &quot;binomial&quot;) Fit a logistic regression model that predicts Direction using Lag1 and Lag2 using all but the first observation. fit &lt;- glm(Direction ~ Lag1 + Lag2, data = Weekly[-1, ], family = &quot;binomial&quot;) Use the model from (b) to predict the direction of the first observation. You can do this by predicting that the first observation will go up if \\(P(\\)Direction=\"Up\" | Lag1 , Lag2\\() &gt; 0.5\\). Was this observation correctly classified? predict(fit, newdata = Weekly[1, , drop = FALSE], type = &quot;response&quot;) &gt; 0.5 ## 1 ## TRUE Yes the observation was correctly classified. Write a for loop from \\(i = 1\\) to \\(i = n\\), where \\(n\\) is the number of observations in the data set, that performs each of the following steps: Fit a logistic regression model using all but the \\(i\\)th observation to predict Direction using Lag1 and Lag2 . Compute the posterior probability of the market moving up for the \\(i\\)th observation. Use the posterior probability for the \\(i\\)th observation in order to predict whether or not the market moves up. Determine whether or not an error was made in predicting the direction for the \\(i\\)th observation. If an error was made, then indicate this as a 1, and otherwise indicate it as a 0. error &lt;- numeric(nrow(Weekly)) for (i in 1:nrow(Weekly)) { fit &lt;- glm(Direction ~ Lag1 + Lag2, data = Weekly[-i, ], family = &quot;binomial&quot;) p &lt;- predict(fit, newdata = Weekly[i, , drop = FALSE], type = &quot;response&quot;) &gt; 0.5 error[i] &lt;- ifelse(p, &quot;Down&quot;, &quot;Up&quot;) == Weekly$Direction[i] } Take the average of the \\(n\\) numbers obtained in (d) in order to obtain the LOOCV estimate for the test error. Comment on the results. mean(error) ## [1] 0.4499541 The LOOCV test error rate is 45% which implies that our predictions are marginally more often correct than not. 5.2.4 Question 8 We will now perform cross-validation on a simulated data set. Generate a simulated data set as follows: &gt; set.seed(1) &gt; x &lt;- rnorm(100) &gt; y &lt;- x - 2 *x^2 + rnorm(100) In this data set, what is \\(n\\) and what is \\(p\\)? Write out the model used to generate the data in equation form. set.seed(1) x &lt;- rnorm(100) y &lt;- x - 2 * x^2 + rnorm(100) \\(n\\) is 100 and \\(p\\) is 1 (there are 100 observations and \\(y\\) is predicted with a single variable \\(x\\)). The model equation is: \\[y = -2x^2 + x + \\epsilon\\]. Create a scatterplot of \\(X\\) against \\(Y\\). Comment on what you find. plot(x, y) \\(y\\) has a (negative) quadratic relationship with \\(x\\). Set a random seed, and then compute the LOOCV errors that result from fitting the following four models using least squares: \\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\) \\(Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\epsilon\\) \\(Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\epsilon\\) \\(Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\beta_4 X^4 + \\epsilon\\). Note you may find it helpful to use the data.frame() function to create a single data set containing both \\(X\\) and \\(Y\\). set.seed(42) dat &lt;- data.frame(x, y) sapply(1:4, function(i) cv.glm(dat, glm(y ~ poly(x, i)))$delta[1]) ## [1] 7.2881616 0.9374236 0.9566218 0.9539049 Repeat (c) using another random seed, and report your results. Are your results the same as what you got in (c)? Why? set.seed(43) dat &lt;- data.frame(x, y) sapply(1:4, function(i) cv.glm(dat, glm(y ~ poly(x, i)))$delta[1]) ## [1] 7.2881616 0.9374236 0.9566218 0.9539049 The results are the same because we are using LOOCV. When doing this, the model is fit leaving each one of the observations out in turn, and thus there is no stochasticity involved. Which of the models in (c) had the smallest LOOCV error? Is this what you expected? Explain your answer. The second model had the smallest LOOCV. This what would be expected since the model to generate the data was quadratic and we are measuring the test (rather than training) error rate to evaluate performance. Comment on the statistical significance of the coefficient estimates that results from fitting each of the models in (c) using least squares. Do these results agree with the conclusions drawn based on the cross-validation results? for (i in 1:4) printCoefmat(coef(summary(glm(y ~ poly(x, i), data = dat)))) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.55002 0.26001 -5.9613 3.954e-08 *** ## poly(x, i) 6.18883 2.60014 2.3802 0.01924 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.550023 0.095803 -16.1792 &lt; 2.2e-16 *** ## poly(x, i)1 6.188826 0.958032 6.4599 4.185e-09 *** ## poly(x, i)2 -23.948305 0.958032 -24.9974 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.550023 0.096263 -16.1019 &lt; 2.2e-16 *** ## poly(x, i)1 6.188826 0.962632 6.4291 4.972e-09 *** ## poly(x, i)2 -23.948305 0.962632 -24.8779 &lt; 2.2e-16 *** ## poly(x, i)3 0.264106 0.962632 0.2744 0.7844 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.550023 0.095905 -16.1620 &lt; 2.2e-16 *** ## poly(x, i)1 6.188826 0.959051 6.4531 4.591e-09 *** ## poly(x, i)2 -23.948305 0.959051 -24.9708 &lt; 2.2e-16 *** ## poly(x, i)3 0.264106 0.959051 0.2754 0.7836 ## poly(x, i)4 1.257095 0.959051 1.3108 0.1931 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We can see that the coefficients in the first model are not highly significant, but all terms (\\(\\beta_0, \\beta_1\\) and \\(\\beta_2\\)) are in the quadratic model. After this, subsequent \\(\\beta_n\\) terms are not significant. Therefore, these results agree with those from cross-validation. 5.2.5 Question 9 We will now consider the Boston housing data set, from the ISLR2 library. Based on this data set, provide an estimate for the population mean of medv. Call this estimate \\(\\hat\\mu\\). (mu &lt;- mean(Boston$medv)) ## [1] 22.53281 Provide an estimate of the standard error of \\(\\hat\\mu\\). Interpret this result. Hint: We can compute the standard error of the sample mean by dividing the sample standard deviation by the square root of the number of observations. sd(Boston$medv) / sqrt(length(Boston$medv)) ## [1] 0.4088611 Now estimate the standard error of \\(\\hat\\mu\\) using the bootstrap. How does this compare to your answer from (b)? set.seed(42) (bs &lt;- boot(Boston$medv, function(v, i) mean(v[i]), 10000)) ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = Boston$medv, statistic = function(v, i) mean(v[i]), ## R = 10000) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 22.53281 0.002175751 0.4029139 The standard error using the bootstrap (0.403) is very close to that obtained from the formula above (0.409). Based on your bootstrap estimate from (c), provide a 95% confidence interval for the mean of medv. Compare it to the results obtained using t.test(Boston$medv). Hint: You can approximate a 95% confidence interval using the formula \\([\\hat\\mu − 2SE(\\hat\\mu), \\hat\\mu + 2SE(\\hat\\mu)].\\) se &lt;- sd(bs$t) c(mu - 2*se, mu + 2*se) ## [1] 21.72698 23.33863 Based on this data set, provide an estimate, \\(\\hat\\mu_{med}\\), for the median value of medv in the population. median(Boston$medv) ## [1] 21.2 We now would like to estimate the standard error of \\(\\hat\\mu_{med}\\). Unfortunately, there is no simple formula for computing the standard error of the median. Instead, estimate the standard error of the median using the bootstrap. Comment on your findings. set.seed(42) boot(Boston$medv, function(v, i) median(v[i]), 10000) ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = Boston$medv, statistic = function(v, i) median(v[i]), ## R = 10000) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 21.2 -0.01331 0.3744634 The estimated standard error of the median is 0.374. This is lower than the standard error of the mean. Based on this data set, provide an estimate for the tenth percentile of medv in Boston census tracts. Call this quantity \\(\\hat\\mu_{0.1}\\). (You can use the quantile() function.) quantile(Boston$medv, 0.1) ## 10% ## 12.75 Use the bootstrap to estimate the standard error of \\(\\hat\\mu_{0.1}\\). Comment on your findings. set.seed(42) boot(Boston$medv, function(v, i) quantile(v[i], 0.1), 10000) ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = Boston$medv, statistic = function(v, i) quantile(v[i], ## 0.1), R = 10000) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 12.75 0.013405 0.497298 We get a standard error of ~0.5. This is higher than the standard error of the median. Nevertheless the standard error is quite small, thus we can be fairly confidence about the value of the 10th percentile. "],["linear-model-selection-and-regularization.html", "6 Linear Model Selection and Regularization 6.1 Conceptual 6.2 Applied", " 6 Linear Model Selection and Regularization 6.1 Conceptual 6.1.1 Question 1 We perform best subset, forward stepwise, and backward stepwise selection on a single data set. For each approach, we obtain \\(p + 1\\) models, containing \\(0, 1, 2, ..., p\\) predictors. Explain your answers: Which of the three models with \\(k\\) predictors has the smallest training RSS? Which of the three models with \\(k\\) predictors has the smallest test RSS? True or False: The predictors in the \\(k\\)-variable model identified by forward stepwise are a subset of the predictors in the (\\(k+1\\))-variable model identified by forward stepwise selection. The predictors in the \\(k\\)-variable model identified by backward stepwise are a subset of the predictors in the \\((k+1)\\)-variable model identified by backward stepwise selection. The predictors in the \\(k\\)-variable model identified by backward stepwise are a subset of the predictors in the \\((k+1)\\)-variable model identified by forward stepwise selection. The predictors in the \\(k\\)-variable model identified by forward stepwise are a subset of the predictors in the \\((k+1)\\)-variable model identified by backward stepwise selection. The predictors in the \\(k\\)-variable model identified by best subset are a subset of the predictors in the \\((k+1)\\)-variable model identified by best subset selection. 6.1.2 Question 2 For parts (a) through (c), indicate which of i. through iv. is correct. Justify your answer. The lasso, relative to least squares, is: More flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance. More flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias. Less flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance. Less flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias. Repeat (a) for ridge regression relative to least squares. Repeat (a) for non-linear methods relative to least squares. 6.1.3 Question 3 Suppose we estimate the regression coefficients in a linear regression model by minimizing: \\[ \\sum_{i=1}^n\\left(y_i - \\beta_0 - \\sum_{j=1}^p\\beta_jx_{ij}\\right)^2 \\textrm{subject to} \\sum_{j=1}^p|\\beta_j| \\le s \\] for a particular value of \\(s\\). For parts (a) through (e), indicate which of i. through v. is correct. Justify your answer. As we increase \\(s\\) from 0, the training RSS will: Increase initially, and then eventually start decreasing in an inverted U shape. Decrease initially, and then eventually start increasing in a U shape. Steadily increase. Steadily decrease. Remain constant. Repeat (a) for test RSS. Repeat (a) for variance. Repeat (a) for (squared) bias. Repeat (a) for the irreducible error. 6.1.4 Question 4 Suppose we estimate the regression coefficients in a linear regression model by minimizing \\[ \\sum_{i=1}^n \\left(y_i - \\beta_0 - \\sum_{j=1}^p\\beta_jx_{ij}\\right)^2 + \\lambda\\sum_{j=1}^p\\beta_j^2 \\] for a particular value of \\(\\lambda\\). For parts (a) through (e), indicate which of i. through v. is correct. Justify your answer. As we increase \\(\\lambda\\) from 0, the training RSS will: Increase initially, and then eventually start decreasing in an inverted U shape. Decrease initially, and then eventually start increasing in a U shape. Steadily increase. Steadily decrease. Remain constant. Repeat (a) for test RSS. Repeat (a) for variance. Repeat (a) for (squared) bias. Repeat (a) for the irreducible error. 6.1.5 Question 5 It is well-known that ridge regression tends to give similar coefficient values to correlated variables, whereas the lasso may give quite different coefficient values to correlated variables. We will now explore this property in a very simple setting. Suppose that \\(n = 2, p = 2, x_{11} = x_{12}, x_{21} = x_{22}\\). Furthermore, suppose that \\(y_1 + y_2 =0\\) and \\(x_{11} + x_{21} = 0\\) and \\(x_{12} + x_{22} = 0\\), so that the estimate for the intercept in a least squares, ridge regression, or lasso model is zero: \\(\\hat{\\beta}_0 = 0\\). Write out the ridge regression optimization problem in this setting. Argue that in this setting, the ridge coefficient estimates satisfy \\(\\hat{\\beta}_1 = \\hat{\\beta}_2\\) Write out the lasso optimization problem in this setting. Argue that in this setting, the lasso coefficients \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\) are not unique—in other words, there are many possible solutions to the optimization problem in (c). Describe these solutions. 6.1.6 Question 6 We will now explore (6.12) and (6.13) further. Consider (6.12) with \\(p = 1\\). For some choice of \\(y_1\\) and \\(\\lambda &gt; 0\\), plot (6.12) as a function of \\(\\beta_1\\). Your plot should confirm that (6.12) is solved by (6.14). Consider (6.13) with \\(p = 1\\). For some choice of \\(y_1\\) and \\(\\lambda &gt; 0\\), plot (6.13) as a function of \\(\\beta_1\\). Your plot should confirm that (6.13) is solved by (6.15). 6.1.7 Question 7 We will now derive the Bayesian connection to the lasso and ridge regression discussed in Section 6.2.2. Suppose that \\(y_i = \\beta_0 + \\sum_{j=1}^p x_{ij}\\beta_j + \\epsilon_i\\) where \\(\\epsilon_1, ..., \\epsilon_n\\) are independent and identically distributed from a \\(N(0, \\sigma^2)\\) distribution. Write out the likelihood for the data. Assume the following prior for \\(\\beta\\): \\(\\beta_1, ..., \\beta_p\\) are independent and identically distributed according to a double-exponential distribution with mean 0 and common scale parameter b: i.e. \\(p(\\beta) = \\frac{1}{2b}\\exp(-|\\beta|/b)\\). Write out the posterior for \\(\\beta\\) in this setting. Argue that the lasso estimate is the mode for \\(\\beta\\) under this posterior distribution. Now assume the following prior for \\(\\beta\\): \\(\\beta_1, ..., \\beta_p\\) are independent and identically distributed according to a normal distribution with mean zero and variance \\(c\\). Write out the posterior for \\(\\beta\\) in this setting. Argue that the ridge regression estimate is both the mode and the mean for \\(\\beta\\) under this posterior distribution. 6.2 Applied 6.2.1 Question 8 In this exercise, we will generate simulated data, and will then use this data to perform best subset selection. Use the rnorm() function to generate a predictor \\(X\\) of length \\(n = 100\\), as well as a noise vector \\(\\epsilon\\) of length \\(n = 100\\). Generate a response vector \\(Y\\) of length \\(n = 100\\) according to the model \\[Y = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\beta_3X^3 + \\epsilon,\\] where \\(\\beta_0, \\beta_1, \\beta_2,\\) and \\(\\beta_3\\) are constants of your choice. Use the regsubsets() function to perform best subset selection in order to choose the best model containing the predictors \\(X, X^2, ..., X^{10}\\). What is the best model obtained according to \\(C_p\\), BIC, and adjusted \\(R^2\\)? Show some plots to provide evidence for your answer, and report the coefficients of the best model obtained. Note you will need to use the data.frame() function to create a single data set containing both \\(X\\) and \\(Y\\). Repeat (c), using forward stepwise selection and also using backwards stepwise selection. How does your answer compare to the results in (c)? Now fit a lasso model to the simulated data, again using \\(X, X^2, ..., X^{10}\\) as predictors. Use cross-validation to select the optimal value of \\(\\lambda\\). Create plots of the cross-validation error as a function of \\(\\lambda\\). Report the resulting coefficient estimates, and discuss the results obtained. Now generate a response vector \\(Y\\) according to the model \\[Y = \\beta_0 + \\beta_7X^7 + \\epsilon,\\] and perform best subset selection and the lasso. Discuss the results obtained. 6.2.2 Question 9 In this exercise, we will predict the number of applications received using the other variables in the College data set. Split the data set into a training set and a test set. Fit a linear model using least squares on the training set, and report the test error obtained. Fit a ridge regression model on the training set, with \\(\\lambda\\) chosen by cross-validation. Report the test error obtained. Fit a lasso model on the training set, with \\(\\lambda\\) chosen by cross- validation. Report the test error obtained, along with the number of non-zero coefficient estimates. Fit a PCR model on the training set, with \\(M\\) chosen by cross-validation. Report the test error obtained, along with the value of \\(M\\) selected by cross-validation. Fit a PLS model on the training set, with \\(M\\) chosen by cross-validation. Report the test error obtained, along with the value of \\(M\\) selected by cross-validation. Comment on the results obtained. How accurately can we predict the number of college applications received? Is there much difference among the test errors resulting from these five approaches? 6.2.3 Question 10 We have seen that as the number of features used in a model increases, the training error will necessarily decrease, but the test error may not. We will now explore this in a simulated data set. Generate a data set with \\(p = 20\\) features, \\(n = 1,000\\) observations, and an associated quantitative response vector generated according to the model \\(Y =X\\beta + \\epsilon\\), where \\(\\beta\\) has some elements that are exactly equal to zero. Split your data set into a training set containing 100 observations and a test set containing 900 observations. Perform best subset selection on the training set, and plot the training set MSE associated with the best model of each size. Plot the test set MSE associated with the best model of each size. For which model size does the test set MSE take on its minimum value? Comment on your results. If it takes on its minimum value for a model containing only an intercept or a model containing all of the features, then play around with the way that you are generating the data in (a) until you come up with a scenario in which the test set MSE is minimized for an intermediate model size. How does the model at which the test set MSE is minimized compare to the true model used to generate the data? Comment on the coefficient values. Create a plot displaying \\(\\sqrt{\\sum_{j=1}^p (\\beta_j - \\hat{\\beta}{}_j^r)^2}\\) for a range of values of \\(r\\), where \\(\\hat{\\beta}{}_j^r\\) is the \\(j\\)th coefficient estimate for the best model containing \\(r\\) coefficients. Comment on what you observe. How does this compare to the test MSE plot from (d)? 6.2.4 Question 11 We will now try to predict per capita crime rate in the Boston data set. Try out some of the regression methods explored in this chapter, such as best subset selection, the lasso, ridge regression, and PCR. Present and discuss results for the approaches that you consider. Propose a model (or set of models) that seem to perform well on this data set, and justify your answer. Make sure that you are evaluating model performance using validation set error, cross-validation, or some other reasonable alternative, as opposed to using training error. Does your chosen model involve all of the features in the data set? Why or why not? "],["moving-beyond-linearity.html", "7 Moving Beyond Linearity 7.1 Conceptual 7.2 Applied", " 7 Moving Beyond Linearity 7.1 Conceptual 7.1.1 Question 1 It was mentioned in the chapter that a cubic regression spline with one knot at \\(\\xi\\) can be obtained using a basis of the form \\(x, x^2, x^3, (x-\\xi)^3_+\\), where \\((x-\\xi)^3_+ = (x-\\xi)^3\\) if \\(x&gt;\\xi\\) and equals 0 otherwise. We will now show that a function of the form \\[ f(x)=\\beta_0 +\\beta_1x+\\beta_2x^2 +\\beta_3x^3 +\\beta_4(x-\\xi)^3_+ \\] is indeed a cubic regression spline, regardless of the values of \\(\\beta_0, \\beta_1, \\beta_2, \\beta_3,\\beta_4\\). Find a cubic polynomial \\[ f_1(x) = a_1 + b_1x + c_1x^2 + d_1x^3 \\] such that \\(f(x) = f_1(x)\\) for all \\(x \\le \\xi\\). Express \\(a_1,b_1,c_1,d_1\\) in terms of \\(\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\beta_4\\). Find a cubic polynomial \\[ f_2(x) = a_2 + b_2x + c_2x^2 + d_2x^3 \\] such that \\(f(x) = f_2(x)\\) for all \\(x &gt; \\xi\\). Express \\(a_2, b_2, c_2, d_2\\) in terms of \\(\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\beta_4\\). We have now established that \\(f(x)\\) is a piecewise polynomial. Show that \\(f_1(\\xi) = f_2(\\xi)\\). That is, \\(f(x)\\) is continuous at \\(\\xi\\). Show that \\(f_1&#39;(\\xi) = f_2&#39;(\\xi)\\). That is, \\(f&#39;(x)\\) is continuous at \\(\\xi\\). Show that \\(f_1&#39;&#39;(\\xi) = f_2&#39;&#39;(\\xi)\\). That is, \\(f&#39;&#39;(x)\\) is continuous at \\(\\xi\\). Therefore, \\(f(x)\\) is indeed a cubic spline. Hint: Parts (d) and (e) of this problem require knowledge of single-variable calculus. As a reminder, given a cubic polynomial \\[f_1(x) = a_1 + b_1x + c_1x^2 + d_1x^3,\\] the first derivative takes the form \\[f_1&#39;(x) = b_1 + 2c_1x + 3d_1x^2\\] and the second derivative takes the form \\[f_1&#39;&#39;(x) = 2c_1 + 6d_1x.\\] 7.1.2 Question 2 Suppose that a curve \\(\\hat{g}\\) is computed to smoothly fit a set of \\(n\\) points using the following formula: \\[ \\DeclareMathOperator*{\\argmin}{arg\\,min} % Jan Hlavacek \\hat{g} = \\argmin_g \\left(\\sum_{i=1}^n (y_i - g(x_i))^2 + \\lambda \\int \\left[ g^{(m)}(x) \\right]^2 dx \\right), \\] where \\(g^{(m)}\\) represents the \\(m\\)th derivative of \\(g\\) (and \\(g^{(0)} = g\\)). Provide example sketches of \\(\\hat{g}\\) in each of the following scenarios. \\(\\lambda=\\infty, m=0\\). \\(\\lambda=\\infty, m=1\\). \\(\\lambda=\\infty, m=2\\). \\(\\lambda=\\infty, m=3\\). \\(\\lambda=0, m=3\\). 7.1.3 Question 3 Suppose we fit a curve with basis functions \\(b_1(X) = X\\), \\(b_2(X) = (X - 1)^2I(X \\geq 1)\\). (Note that \\(I(X \\geq 1)\\) equals 1 for \\(X \\geq 1\\) and 0 otherwise.) We fit the linear regression model \\[Y = \\beta_0 +\\beta_1b_1(X) + \\beta_2b_2(X) + \\epsilon,\\] and obtain coefficient estimates \\(\\hat{\\beta}_0 = 1, \\hat{\\beta}_1 = 1, \\hat{\\beta}_2 = -2\\). Sketch the estimated curve between \\(X = -2\\) and \\(X = 6\\). Note the intercepts, slopes, and other relevant information. 7.1.4 Question 4 Suppose we fit a curve with basis functions \\(b_1(X) = I(0 \\leq X \\leq 2) - (X -1)I(1 \\leq X \\leq 2),\\) \\(b_2(X) = (X -3)I(3 \\leq X \\leq 4) + I(4 \\lt X \\leq 5)\\). We fit the linear regression model \\[Y = \\beta_0 +\\beta_1b_1(X) + \\beta_2b_2(X) + \\epsilon,\\] and obtain coefficient estimates \\(\\hat{\\beta}_0 = 1, \\hat{\\beta}_1 = 1, \\hat{\\beta}_2 = 3\\). Sketch the estimated curve between \\(X = -2\\) and \\(X = 2\\). Note the intercepts, slopes, and other relevant information. 7.1.5 Question 5 Consider two curves, \\(\\hat{g}\\) and \\(\\hat{g}_2\\), defined by \\[ \\hat{g}_1 = \\argmin_g \\left(\\sum_{i=1}^n (y_i - g(x_i))^2 + \\lambda \\int \\left[ g^{(3)}(x) \\right]^2 dx \\right), \\] \\[ \\hat{g}_2 = \\argmin_g \\left(\\sum_{i=1}^n (y_i - g(x_i))^2 + \\lambda \\int \\left[ g^{(4)}(x) \\right]^2 dx \\right), \\] where \\(g^{(m)}\\) represents the \\(m\\)th derivative of \\(g\\). As \\(\\lambda \\to \\infty\\), will \\(\\hat{g}_1\\) or \\(\\hat{g}_2\\) have the smaller training RSS? As \\(\\lambda \\to \\infty\\), will \\(\\hat{g}_1\\) or \\(\\hat{g}_2\\) have the smaller test RSS? For \\(\\lambda = 0\\), will \\(\\hat{g}_1\\) or \\(\\hat{g}_2\\) have the smaller training and test RSS? 7.2 Applied 7.2.1 Question 6 In this exercise, you will further analyze the Wage data set considered throughout this chapter. Perform polynomial regression to predict wage using age. Use cross-validation to select the optimal degree \\(d\\) for the polynomial. What degree was chosen, and how does this compare to the results of hypothesis testing using ANOVA? Make a plot of the resulting polynomial fit to the data. Fit a step function to predict wage using age, and perform cross-validation to choose the optimal number of cuts. Make a plot of the fit obtained. 7.2.2 Question 7 The Wage data set contains a number of other features not explored in this chapter, such as marital status (maritl), job class (jobclass), and others. Explore the relationships between some of these other predictors and wage, and use non-linear fitting techniques in order to fit flexible models to the data. Create plots of the results obtained, and write a summary of your findings. 7.2.3 Question 8 Fit some of the non-linear models investigated in this chapter to the Auto data set. Is there evidence for non-linear relationships in this data set? Create some informative plots to justify your answer. 7.2.4 Question 9 This question uses the variables dis (the weighted mean of distances to five Boston employment centers) and nox (nitrogen oxides concentration in parts per 10 million) from the Boston data. We will treat dis as the predictor and nox as the response. Use the poly() function to fit a cubic polynomial regression to predict nox using dis. Report the regression output, and plot the resulting data and polynomial fits. Plot the polynomial fits for a range of different polynomial degrees (say, from 1 to 10), and report the associated residual sum of squares. Perform cross-validation or another approach to select the optimal degree for the polynomial, and explain your results. Use the bs() function to fit a regression spline to predict nox using dis. Report the output for the fit using four degrees of freedom. How did you choose the knots? Plot the resulting fit. Now fit a regression spline for a range of degrees of freedom, and plot the resulting fits and report the resulting RSS. Describe the results obtained. Perform cross-validation or another approach in order to select the best degrees of freedom for a regression spline on this data. Describe your results. 7.2.5 Question 10 This question relates to the College data set. Split the data into a training set and a test set. Using out-of-state tuition as the response and the other variables as the predictors, perform forward stepwise selection on the training set in order to identify a satisfactory model that uses just a subset of the predictors. Fit a GAM on the training data, using out-of-state tuition as the response and the features selected in the previous step as the predictors. Plot the results, and explain your findings. Evaluate the model obtained on the test set, and explain the results obtained. For which variables, if any, is there evidence of a non-linear relationship with the response? 7.2.6 Question 11 In Section 7.7, it was mentioned that GAMs are generally fit using a backfitting approach. The idea behind backfitting is actually quite simple. We will now explore backfitting in the context of multiple linear regression. Suppose that we would like to perform multiple linear regression, but we do not have software to do so. Instead, we only have software to perform simple linear regression. Therefore, we take the following iterative approach: we repeatedly hold all but one coefficient estimate fixed at its current value, and update only that coefficient estimate using a simple linear regression. The process is continued until convergence—that is, until the coefficient estimates stop changing. We now try this out on a toy example. Generate a response \\(Y\\) and two predictors \\(X_1\\) and \\(X_2\\), with \\(n = 100\\). Initialize \\(\\hat{\\beta}_1\\) to take on a value of your choice. It does not matter 1 what value you choose. Keeping \\(\\hat{\\beta}_1\\) fixed, fit the model \\[Y - \\hat{\\beta}_1X_1 = \\beta_0 + \\beta_2X_2 + \\epsilon.\\] You can do this as follows: &gt; a &lt;- y - beta1 * x1 &gt; beta2 &lt;- lm(a ~ x2)$coef[2] Keeping \\(\\hat{\\beta}_2\\) fixed, fit the model \\[Y - \\hat{\\beta}_2X_2 = \\beta_0 + \\beta_1 X_1 + \\epsilon.\\] You can do this as follows: &gt; a &lt;- y - beta2 * x2 &gt; beta1 &lt;- lm(a ~ x1)$coef[2] Write a for loop to repeat (c) and (d) 1,000 times. Report the estimates of \\(\\hat{\\beta}_0, \\hat{\\beta}_1,\\) and \\(\\hat{\\beta}_2\\) at each iteration of the for loop. Create a plot in which each of these values is displayed, with \\(\\hat{\\beta}_0, \\hat{\\beta}_1,\\) and \\(\\hat{\\beta}_2\\) each shown in a different color. Compare your answer in (e) to the results of simply performing multiple linear regression to predict \\(Y\\) using \\(X_1\\) and \\(X_2\\). Use the abline() function to overlay those multiple linear regression coefficient estimates on the plot obtained in (e). On this data set, how many backfitting iterations were required in order to obtain a “good” approximation to the multiple regression coefficient estimates? 7.2.7 Question 12 This problem is a continuation of the previous exercise. In a toy example with \\(p = 100\\), show that one can approximate the multiple linear regression coefficient estimates by repeatedly performing simple linear regression in a backfitting procedure. How many backfitting iterations are required in order to obtain a “good” approximation to the multiple regression coefficient estimates? Create a plot to justify your answer. "],["tree-based-methods.html", "8 Tree-Based Methods 8.1 Conceptual 8.2 Applied", " 8 Tree-Based Methods 8.1 Conceptual 8.1.1 Question 1 Draw an example (of your own invention) of a partition of two-dimensional feature space that could result from recursive binary splitting. Your example should contain at least six regions. Draw a decision tree corresponding to this partition. Be sure to label all aspects of your figures, including the regions \\(R_1, R_2, ...,\\) the cutpoints \\(t_1, t_2, ...,\\) and so forth. Hint: Your result should look something like Figures 8.1 and 8.2. 8.1.2 Question 2 It is mentioned in Section 8.2.3 that boosting using depth-one trees (or stumps) leads to an additive model: that is, a model of the form \\[ f(X) = \\sum_{j=1}^p f_j(X_j). \\] Explain why this is the case. You can begin with (8.12) in Algorithm 8.2. 8.1.3 Question 3 Consider the Gini index, classification error, and cross-entropy in a simple classification setting with two classes. Create a single plot that displays each of these quantities as a function of \\(\\hat{p}_{m1}\\). The \\(x\\)-axis should display \\(\\hat{p}_{m1}\\), ranging from 0 to 1, and the \\(y\\)-axis should display the value of the Gini index, classification error, and entropy. Hint: In a setting with two classes, \\(\\hat{p}_{m1} = 1 - \\hat{p}_{m2}\\). You could make this plot by hand, but it will be much easier to make in R. 8.1.4 Question 4 This question relates to the plots in Figure 8.12. Sketch the tree corresponding to the partition of the predictor space illustrated in the left-hand panel of Figure 8.12. The numbers inside the boxes indicate the mean of \\(Y\\) within each region. Create a diagram similar to the left-hand panel of Figure 8.12, using the tree illustrated in the right-hand panel of the same figure. You should divide up the predictor space into the correct regions, and indicate the mean for each region. 8.1.5 Question 5 Suppose we produce ten bootstrapped samples from a data set containing red and green classes. We then apply a classification tree to each bootstrapped sample and, for a specific value of \\(X\\), produce 10 estimates of \\(P(\\textrm{Class is Red}|X)\\): \\[0.1, 0.15, 0.2, 0.2, 0.55, 0.6, 0.6, 0.65, 0.7, \\textrm{and } 0.75.\\] There are two common ways to combine these results together into a single class prediction. One is the majority vote approach discussed in this chapter. The second approach is to classify based on the average probability. In this example, what is the final classification under each of these two approaches? 8.1.6 Question 6 Provide a detailed explanation of the algorithm that is used to fit a regression tree. 8.2 Applied 8.2.1 Question 7 In the lab, we applied random forests to the Boston data using mtry = 6 and using ntree = 25 and ntree = 500. Create a plot displaying the test error resulting from random forests on this data set for a more comprehensive range of values for mtry and ntree. You can model your plot after Figure 8.10. Describe the results obtained. 8.2.2 Question 8 In the lab, a classification tree was applied to the Carseats data set after converting Sales into a qualitative response variable. Now we will seek to predict Sales using regression trees and related approaches, treating the response as a quantitative variable. Split the data set into a training set and a test set. Fit a regression tree to the training set. Plot the tree, and interpret the results. What test error rate do you obtain? Use cross-validation in order to determine the optimal level of tree complexity. Does pruning the tree improve the test error rate? Use the bagging approach in order to analyze this data. What test error rate do you obtain? Use the importance() function to determine which variables are most important. Use random forests to analyze this data. What test error rate do you obtain? Use the importance() function to determine which variables are most important. Describe the effect of \\(m\\), the number of variables considered at each split, on the error rate obtained. Now analyze the data using BART, and report your results. 8.2.3 Question 9 This problem involves the OJ data set which is part of the ISLR2 package. Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations. Fit a tree to the training data, with Purchase as the response and the other variables except for Buy as predictors. Use the summary() function to produce summary statistics about the tree, and describe the results obtained. What is the training error rate? How many terminal nodes does the tree have? Type in the name of the tree object in order to get a detailed text output. Pick one of the terminal nodes, and interpret the information displayed. Create a plot of the tree, and interpret the results. Predict the response on the test data, and produce a confusion matrix comparing the test labels to the predicted test labels. What is the test error rate? Apply the cv.tree() function to the training set in order to determine the optimal tree size. Produce a plot with tree size on the \\(x\\)-axis and cross-validated classification error rate on the \\(y\\)-axis. Which tree size corresponds to the lowest cross-validated classification error rate? Produce a pruned tree corresponding to the optimal tree size obtained using cross-validation. If cross-validation does not lead to selection of a pruned tree, then create a pruned tree with five terminal nodes. Compare the training error rates between the pruned and unpruned trees. Which is higher? Compare the test error rates between the pruned and unpruned trees. Which is higher? 8.2.4 Question 10 We now use boosting to predict Salary in the Hitters data set. Remove the observations for whom the salary information is unknown, and then log-transform the salaries. Create a training set consisting of the first 200 observations, and a test set consisting of the remaining observations. Perform boosting on the training set with 1,000 trees for a range of values of the shrinkage parameter \\(\\lambda\\). Produce a plot with different shrinkage values on the \\(x\\)-axis and the corresponding training set MSE on the \\(y\\)-axis. Produce a plot with different shrinkage values on the \\(x\\)-axis and the corresponding test set MSE on the \\(y\\)-axis. Compare the test MSE of boosting to the test MSE that results from applying two of the regression approaches seen in Chapters 3 and 6. Which variables appear to be the most important predictors in the boosted model? Now apply bagging to the training set. What is the test set MSE for this approach? 8.2.5 Question 11 This question uses the Caravan data set. Create a training set consisting of the first 1,000 observations, and a test set consisting of the remaining observations. Fit a boosting model to the training set with Purchase as the response and the other variables as predictors. Use 1,000 trees, and a shrinkage value of 0.01. Which predictors appear to be the most important? Use the boosting model to predict the response on the test data. Predict that a person will make a purchase if the estimated probability of purchase is greater than 20%. Form a confusion matrix. What fraction of the people predicted to make a purchase do in fact make one? How does this compare with the results obtained from applying KNN or logistic regression to this data set? 8.2.6 Question 12 Apply boosting, bagging, random forests and BART to a data set of your choice. Be sure to fit the models on a training set and to evaluate their performance on a test set. How accurate are the results compared to simple methods like linear or logistic regression? Which of these approaches yields the best performance? "],["support-vector-machines.html", "9 Support Vector Machines 9.1 Conceptual 9.2 Applied", " 9 Support Vector Machines 9.1 Conceptual 9.1.1 Question 1 This problem involves hyperplanes in two dimensions. Sketch the hyperplane \\(1 + 3X_1 − X_2 = 0\\). Indicate the set of points for which \\(1 + 3X_1 − X_2 &gt; 0\\), as well as the set of points for which \\(1 + 3X_1 − X_2 &lt; 0\\). On the same plot, sketch the hyperplane \\(−2 + X_1 + 2X_2 = 0\\). Indicate the set of points for which \\(−2 + X_1 + 2X_2 &gt; 0\\), as well as the set of points for which \\(−2 + X_1 + 2X_2 &lt; 0\\). 9.1.2 Question 2 We have seen that in \\(p = 2\\) dimensions, a linear decision boundary takes the form \\(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 = 0\\). We now investigate a non-linear decision boundary. Sketch the curve \\[(1+X_1)^2 +(2−X_2)^2 = 4\\]. On your sketch, indicate the set of points for which \\[(1 + X_1)^2 + (2 − X_2)^2 &gt; 4,\\] as well as the set of points for which \\[(1 + X_1)^2 + (2 − X_2)^2 \\leq 4.\\] Suppose that a classifier assigns an observation to the blue class if \\[(1 + X_1)^2 + (2 − X_2)^2 &gt; 4,\\] and to the red class otherwise. To what class is the observation \\((0, 0)\\) classified? \\((−1, 1)\\)? \\((2, 2)\\)? \\((3, 8)\\)? Argue that while the decision boundary in (c) is not linear in terms of \\(X_1\\) and \\(X_2\\), it is linear in terms of \\(X_1\\), \\(X_1^2\\), \\(X_2\\), and \\(X_2^2\\). 9.1.3 Question 3 Here we explore the maximal margin classifier on a toy data set. We are given \\(n = 7\\) observations in \\(p = 2\\) dimensions. For each observation, there is an associated class label. Obs. \\(X_1\\) \\(X_2\\) \\(Y\\) 1 3 4 Red 2 2 2 Red 3 4 4 Red 4 1 4 Red 5 2 1 Blue 6 4 3 Blue 7 4 1 Blue Sketch the observations. Sketch the optimal separating hyperplane, and provide the equation for this hyperplane (of the form (9.1)). Describe the classification rule for the maximal margin classifier. It should be something along the lines of “Classify to Red if \\(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 &gt; 0\\), and classify to Blue otherwise.” Provide the values for \\(\\beta_0, \\beta_1,\\) and \\(\\beta_2\\). On your sketch, indicate the margin for the maximal margin hyperplane. Indicate the support vectors for the maximal margin classifier. Argue that a slight movement of the seventh observation would not affect the maximal margin hyperplane. Sketch a hyperplane that is not the optimal separating hyperplane, and provide the equation for this hyperplane. Draw an additional observation on the plot so that the two classes are no longer separable by a hyperplane. 9.2 Applied 9.2.1 Question 4 Generate a simulated two-class data set with 100 observations and two features in which there is a visible but non-linear separation between the two classes. Show that in this setting, a support vector machine with a polynomial kernel (with degree greater than 1) or a radial kernel will outperform a support vector classifier on the training data. Which technique performs best on the test data? Make plots and report training and test error rates in order to back up your assertions. 9.2.2 Question 5 We have seen that we can fit an SVM with a non-linear kernel in order to perform classification using a non-linear decision boundary. We will now see that we can also obtain a non-linear decision boundary by performing logistic regression using non-linear transformations of the features. Generate a data set with \\(n = 500\\) and \\(p = 2\\), such that the observations belong to two classes with a quadratic decision boundary between them. For instance, you can do this as follows: &gt; x1 &lt;- runif(500) - 0.5 &gt; x2 &lt;- runif(500) - 0.5 &gt; y &lt;- 1 * (x1^2 - x2^2 &gt; 0) Plot the observations, colored according to their class labels. Your plot should display \\(X_1\\) on the \\(x\\)-axis, and \\(X_2\\) on the \\(y\\)-axis. Fit a logistic regression model to the data, using \\(X_1\\) and \\(X_2\\) as predictors. Apply this model to the training data in order to obtain a predicted class label for each training observation. Plot the observations, colored according to the predicted class labels. The decision boundary should be linear. Now fit a logistic regression model to the data using non-linear functions of \\(X_1\\) and \\(X_2\\) as predictors (e.g. \\(X_1^2, X_1 \\times X_2, \\log(X_2),\\) and so forth). Apply this model to the training data in order to obtain a predicted class label for each training observation. Plot the observations, colored according to the predicted class labels. The decision boundary should be obviously non-linear. If it is not, then repeat (a)-(e) until you come up with an example in which the predicted class labels are obviously non-linear. Fit a support vector classifier to the data with \\(X_1\\) and \\(X_2\\) as predictors. Obtain a class prediction for each training observation. Plot the observations, colored according to the predicted class labels. Fit a SVM using a non-linear kernel to the data. Obtain a class prediction for each training observation. Plot the observations, colored according to the predicted class labels. Comment on your results. 9.2.3 Question 6 At the end of Section 9.6.1, it is claimed that in the case of data that is just barely linearly separable, a support vector classifier with a small value of cost that misclassifies a couple of training observations may perform better on test data than one with a huge value of cost that does not misclassify any training observations. You will now investigate this claim. Generate two-class data with \\(p = 2\\) in such a way that the classes are just barely linearly separable. Compute the cross-validation error rates for support vector classifiers with a range of cost values. How many training errors are misclassified for each value of cost considered, and how does this relate to the cross-validation errors obtained? Generate an appropriate test data set, and compute the test errors corresponding to each of the values of cost considered. Which value of cost leads to the fewest test errors, and how does this compare to the values of cost that yield the fewest training errors and the fewest cross-validation errors? Discuss your results. 9.2.4 Question 7 In this problem, you will use support vector approaches in order to predict whether a given car gets high or low gas mileage based on the Auto data set. Create a binary variable that takes on a 1 for cars with gas mileage above the median, and a 0 for cars with gas mileage below the median. Fit a support vector classifier to the data with various values of cost, in order to predict whether a car gets high or low gas mileage. Report the cross-validation errors associated with different values of this parameter. Comment on your results. Note you will need to fit the classifier without the gas mileage variable to produce sensible results. Now repeat (b), this time using SVMs with radial and polynomial basis kernels, with different values of gamma and degree and cost. Comment on your results. Make some plots to back up your assertions in (b) and (c). Hint: In the lab, we used the plot() function for svm objects only in cases with \\(p = 2\\). When \\(p &gt; 2\\), you can use the plot() function to create plots displaying pairs of variables at a time. Essentially, instead of typing &gt; plot(svmfit, dat) where svmfit contains your fitted model and dat is a data frame containing your data, you can type &gt; plot(svmfit, dat, x1 ∼ x4) in order to plot just the first and fourth variables. However, you must replace x1 and x4 with the correct variable names. To find out more, type ?plot.svm. 9.2.5 Question 8 This problem involves the OJ data set which is part of the ISLR2 package. Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations. Fit a support vector classifier to the training data using cost = 0.01, with Purchase as the response and the other variables as predictors. Use the summary() function to produce summary statistics, and describe the results obtained. What are the training and test error rates? Use the tune() function to select an optimal cost. Consider values in the range 0.01 to 10. Compute the training and test error rates using this new value for cost. Repeat parts (b) through (e) using a support vector machine with a radial kernel. Use the default value for gamma. Repeat parts (b) through (e) using a support vector machine with a polynomial kernel. Set degree = 2. Overall, which approach seems to give the best results on this data? "],["deep-learning.html", "10 Deep Learning 10.1 Conceptual 10.2 Applied", " 10 Deep Learning 10.1 Conceptual 10.1.1 Question 1 Consider a neural network with two hidden layers: \\(p = 4\\) input units, 2 units in the first hidden layer, 3 units in the second hidden layer, and a single output. Draw a picture of the network, similar to Figures 10.1 or 10.4. Write out an expression for \\(f(X)\\), assuming ReLU activation functions. Be as explicit as you can! Now plug in some values for the coefficients and write out the value of \\(f(X)\\). How many parameters are there? 10.1.2 Question 2 Consider the softmax function in (10.13) (see also (4.13) on page 141) for modeling multinomial probabilities. In (10.13), show that if we add a constant \\(c\\) to each of the \\(z_l\\), then the probability is unchanged. In (4.13), show that if we add constants \\(c_j\\), \\(j = 0,1,...,p\\), to each of the corresponding coefficients for each of the classes, then the predictions at any new point \\(x\\) are unchanged. This shows that the softmax function is over-parametrized. However, regularization and SGD typically constrain the solutions so that this is not a problem. 10.1.3 Question 3 Show that the negative multinomial log-likelihood (10.14) is equivalent to the negative log of the likelihood expression (4.5) when there are \\(M = 2\\) classes. 10.1.4 Question 4 Consider a CNN that takes in \\(32 \\times 32\\) grayscale images and has a single convolution layer with three \\(5 \\times 5\\) convolution filters (without boundary padding). Draw a sketch of the input and first hidden layer similar to Figure 10.8. How many parameters are in this model? Explain how this model can be thought of as an ordinary feed-forward neural network with the individual pixels as inputs, and with constraints on the weights in the hidden units. What are the constraints? If there were no constraints, then how many weights would there be in the ordinary feed-forward neural network in (c)? 10.1.5 Question 5 In Table 10.2 on page 433, we see that the ordering of the three methods with respect to mean absolute error is different from the ordering with respect to test set \\(R^2\\). How can this be? 10.2 Applied 10.2.1 Question 6 Consider the simple function \\(R(\\beta) = sin(\\beta) + \\beta/10\\). Draw a graph of this function over the range \\(\\beta \\in [−6, 6]\\). What is the derivative of this function? Given \\(\\beta^0 = 2.3\\), run gradient descent to find a local minimum of \\(R(\\beta)\\) using a learning rate of \\(\\rho = 0.1\\). Show each of \\(\\beta^0, \\beta^1, ...\\) in your plot, as well as the final answer. Repeat with \\(\\beta^0 = 1.4\\). 10.2.2 Question 7 Fit a neural network to the Default data. Use a single hidden layer with 10 units, and dropout regularization. Have a look at Labs 10.9.1–-10.9.2 for guidance. Compare the classification performance of your model with that of linear logistic regression. 10.2.3 Question 8 From your collection of personal photographs, pick 10 images of animals (such as dogs, cats, birds, farm animals, etc.). If the subject does not occupy a reasonable part of the image, then crop the image. Now use a pretrained image classification CNN as in Lab 10.9.4 to predict the class of each of your images, and report the probabilities for the top five predicted classes for each image. 10.2.4 Question 9 Fit a lag-5 autoregressive model to the NYSE data, as described in the text and Lab 10.9.6. Refit the model with a 12-level factor representing the month. Does this factor improve the performance of the model? 10.2.5 Question 10 In Section 10.9.6, we showed how to fit a linear AR model to the NYSE data using the lm() function. However, we also mentioned that we can “flatten” the short sequences produced for the RNN model in order to fit a linear AR model. Use this latter approach to fit a linear AR model to the NYSE data. Compare the test \\(R^2\\) of this linear AR model to that of the linear AR model that we fit in the lab. What are the advantages/disadvantages of each approach? 10.2.6 Question 11 Repeat the previous exercise, but now fit a nonlinear AR model by “flattening” the short sequences produced for the RNN model. 10.2.7 Question 12 Consider the RNN fit to the NYSE data in Section 10.9.6. Modify the code to allow inclusion of the variable day_of_week, and fit the RNN. Compute the test \\(R^2\\). 10.2.8 Question 13 Repeat the analysis of Lab 10.9.5 on the IMDb data using a similarly structured neural network. There we used a dictionary of size 10,000. Consider the effects of varying the dictionary size. Try the values 1000, 3000, 5000, and 10,000, and compare the results. "],["survival-analysis-and-censored-data.html", "11 Survival Analysis and Censored Data 11.1 Conceptual 11.2 Applied", " 11 Survival Analysis and Censored Data 11.1 Conceptual 11.1.1 Question 1 For each example, state whether or not the censoring mechanism is independent. Justify your answer. In a study of disease relapse, due to a careless research scientist, all patients whose phone numbers begin with the number “2” are lost to follow up. In a study of longevity, a formatting error causes all patient ages that exceed 99 years to be lost (i.e. we know that those patients are more than 99 years old, but we do not know their exact ages). Hospital A conducts a study of longevity. However, very sick patients tend to be transferred to Hospital B, and are lost to follow up. In a study of unemployment duration, the people who find work earlier are less motivated to stay in touch with study investigators, and therefore are more likely to be lost to follow up. In a study of pregnancy duration, women who deliver their babies pre-term are more likely to do so away from their usual hospital, and thus are more likely to be censored, relative to women who deliver full-term babies. A researcher wishes to model the number of years of education of the residents of a small town. Residents who enroll in college out of town are more likely to be lost to follow up, and are also more likely to attend graduate school, relative to those who attend college in town. Researchers conduct a study of disease-free survival (i.e. time until disease relapse following treatment). Patients who have not relapsed within five years are considered to be cured, and thus their survival time is censored at five years. We wish to model the failure time for some electrical component. This component can be manufactured in Iowa or in Pittsburgh, with no difference in quality. The Iowa factory opened five years ago, and so components manufactured in Iowa are censored at five years. The Pittsburgh factory opened two years ago, so those components are censored at two years. We wish to model the failure time of an electrical component made in two different factories, one of which opened before the other. We have reason to believe that the components manufactured in the factory that opened earlier are of higher quality. 11.1.2 Question 2 We conduct a study with \\(n = 4\\) participants who have just purchased cell phones, in order to model the time until phone replacement. The first participant replaces her phone after 1.2 years. The second participant still has not replaced her phone at the end of the two-year study period. The third participant changes her phone number and is lost to follow up (but has not yet replaced her phone) 1.5 years into the study. The fourth participant replaces her phone after 0.2 years. For each of the four participants (\\(i = 1,..., 4\\)), answer the following questions using the notation introduced in Section 11.1: Is the participant’s cell phone replacement time censored? Is the value of \\(c_i\\) known, and if so, then what is it? Is the value of \\(t_i\\) known, and if so, then what is it? Is the value of \\(y_i\\) known, and if so, then what is it? Is the value of \\(\\delta_i\\) known, and if so, then what is it? 11.1.3 Question 3 For the example in Exercise 2, report the values of \\(K\\), \\(d_1,...,d_K\\), \\(r_1,...,r_K\\), and \\(q_1,...,q_K\\), where this notation was defined in Section 11.3. 11.1.4 Question 4 This problem makes use of the Kaplan-Meier survival curve displayed in Figure 11.9. The raw data that went into plotting this survival curve is given in Table 11.4. The covariate column of that table is not needed for this problem. What is the estimated probability of survival past 50 days? Write out an analytical expression for the estimated survival function. For instance, your answer might be something along the lines of \\[ \\hat{S}(t) = \\begin{cases} 0.8, &amp; \\text{if } t &lt; 31\\\\ 0.5, &amp; \\text{if } 31 \\le t &lt; 77\\\\ 0.22 &amp; \\text{if } 77 \\le t \\end{cases} \\] (The previous equation is for illustration only: it is not the correct answer!) 11.1.5 Question 5 Sketch the survival function given by the equation \\[ \\hat{S}(t) = \\begin{cases} 0.8, &amp; \\text{if } t &lt; 31\\\\ 0.5, &amp; \\text{if } 31 \\le t &lt; 77\\\\ 0.22 &amp; \\text{if } 77 \\le t \\end{cases} \\] Your answer should look something like Figure 11.9. 11.1.6 Question 6 This problem makes use of the data displayed in Figure 11.1. In completing this problem, you can refer to the observation times as \\(y_1,...,y_4\\). The ordering of these observation times can be seen from Figure 11.1; their exact values are not required. Report the values of \\(\\delta_1,...,\\delta_4\\), \\(K\\), \\(d_1,...,d_K\\), \\(r_1,...,r_K\\), and \\(q_1,...,q_K\\). The relevant notation is defined in Sections 11.1 and 11.3. Sketch the Kaplan-Meier survival curve corresponding to this data set. (You do not need to use any software to do this—you can sketch it by hand using the results obtained in (a).) Based on the survival curve estimated in (b), what is the probability that the event occurs within 200 days? What is the probability that the event does not occur within 310 days? Write out an expression for the estimated survival curve from (b). 11.1.7 Question 7 In this problem, we will derive (11.5) and (11.6), which are needed for the construction of the log-rank test statistic (11.8). Recall the notation in Table 11.1. Assume that there is no difference between the survival functions of the two groups. Then we can think of \\(q_{1k}\\) as the number of failures if we draw $r_{1k} observations, without replacement, from a risk set of \\(r_k\\) observations that contains a total of \\(q_k\\) failures. Argue that \\(q_{1k}\\) follows a hypergeometric distribution. Write the parameters of this distribution in terms of \\(r_{1k}\\), \\(r_k\\), and \\(q_k\\). Given your previous answer, and the properties of the hyper-geometric distribution, what are the mean and variance of \\(q_{1k}\\)? Compare your answer to (11.5) and (11.6). 11.1.8 Question 8 Recall that the survival function \\(S(t)\\), the hazard function \\(h(t)\\), and the density function \\(f(t)\\) are defined in (11.2), (11.9), and (11.11), respectively. Furthermore, define \\(F(t) = 1 − S(t)\\). Show that the following relationships hold: \\[ f(t) = dF(t)/dt \\\\ S(t) = \\exp\\left(-\\int_0^t h(u)du\\right) \\] 11.1.9 Question 9 In this exercise, we will explore the consequences of assuming that the survival times follow an exponential distribution. Suppose that a survival time follows an \\(Exp(\\lambda)\\) distribution, so that its density function is \\(f(t) = \\lambda\\exp(−\\lambda t)\\). Using the relationships provided in Exercise 8, show that \\(S(t) = \\exp(−\\lambda t)\\). Now suppose that each of \\(n\\) independent survival times follows an \\(Exp(\\lambda)\\) distribution. Write out an expression for the likelihood function (11.13). Show that the maximum likelihood estimator for \\(\\lambda\\) is \\[ \\hat\\lambda = \\sum_{i=1}^n \\delta_i / \\sum_{i=1}^n y_i. \\] Use your answer to (c) to derive an estimator of the mean survival time. Hint: For (d), recall that the mean of an \\(Exp(\\lambda)\\) random variable is \\(1/\\lambda\\). 11.2 Applied 11.2.1 Question 10 This exercise focuses on the brain tumor data, which is included in the ISLR2 R library. Plot the Kaplan-Meier survival curve with ±1 standard error bands, using the survfit() function in the survival package. Draw a bootstrap sample of size \\(n = 88\\) from the pairs (\\(y_i\\), \\(\\delta_i\\)), and compute the resulting Kaplan-Meier survival curve. Repeat this process \\(B = 200\\) times. Use the results to obtain an estimate of the standard error of the Kaplan-Meier survival curve at each timepoint. Compare this to the standard errors obtained in (a). Fit a Cox proportional hazards model that uses all of the predictors to predict survival. Summarize the main findings. Stratify the data by the value of ki. (Since only one observation has ki=40, you can group that observation together with the observations that have ki=60.) Plot Kaplan-Meier survival curves for each of the five strata, adjusted for the other predictors. 11.2.2 Question 11 This example makes use of the data in Table 11.4. Create two groups of observations. In Group 1, \\(X &lt; 2\\), whereas in Group 2, \\(X \\ge 2\\). Plot the Kaplan-Meier survival curves corresponding to the two groups. Be sure to label the curves so that it is clear which curve corresponds to which group. By eye, does there appear to be a difference between the two groups’ survival curves? Fit Cox’s proportional hazards model, using the group indicator as a covariate. What is the estimated coefficient? Write a sentence providing the interpretation of this coefficient, in terms of the hazard or the instantaneous probability of the event. Is there evidence that the true coefficient value is non-zero? Recall from Section 11.5.2 that in the case of a single binary covariate, the log-rank test statistic should be identical to the score statistic for the Cox model. Conduct a log-rank test to determine whether there is a difference between the survival curves for the two groups. How does the p-value for the log-rank test statistic compare to the \\(p\\)-value for the score statistic for the Cox model from (b)? "],["unsupervised-learning.html", "12 Unsupervised Learning 12.1 Conceptual 12.2 Applied", " 12 Unsupervised Learning 12.1 Conceptual 12.1.1 Question 1 This problem involves the \\(K\\)-means clustering algorithm. Prove (12.18). On the basis of this identity, argue that the \\(K\\)-means clustering algorithm (Algorithm 12.2) decreases the objective (12.17) at each iteration. 12.1.2 Question 2 Suppose that we have four observations, for which we compute a dissimilarity matrix, given by \\[\\begin{bmatrix} &amp; 0.3 &amp; 0.4 &amp; 0.7 \\\\ 0.3 &amp; &amp; 0.5 &amp; 0.8 \\\\ 0.4 &amp; 0.5 &amp; &amp; 0.45 \\\\ 0.7 &amp; 0.8 &amp; 0.45 &amp; \\\\ \\end{bmatrix}\\] For instance, the dissimilarity between the first and second observations is 0.3, and the dissimilarity between the second and fourth observations is 0.8. On the basis of this dissimilarity matrix, sketch the dendrogram that results from hierarchically clustering these four observations using complete linkage. Be sure to indicate on the plot the height at which each fusion occurs, as well as the observations corresponding to each leaf in the dendrogram. Repeat (a), this time using single linkage clustering. Suppose that we cut the dendrogram obtained in (a) such that two clusters result. Which observations are in each cluster? Suppose that we cut the dendrogram obtained in (b) such that two clusters result. Which observations are in each cluster? It is mentioned in the chapter that at each fusion in the dendrogram, the position of the two clusters being fused can be swapped without changing the meaning of the dendrogram. Draw a dendrogram that is equivalent to the dendrogram in (a), for which two or more of the leaves are repositioned, but for which the meaning of the dendrogram is the same. 12.1.3 Question 3 In this problem, you will perform \\(K\\)-means clustering manually, with \\(K = 2\\), on a small example with \\(n = 6\\) observations and \\(p = 2\\) features. The observations are as follows. Obs. \\(X_1\\) \\(X_2\\) 1 1 4 2 1 3 3 0 4 4 5 1 5 6 2 6 4 0 Plot the observations. Randomly assign a cluster label to each observation. You can use the sample() command in R to do this. Report the cluster labels for each observation. Compute the centroid for each cluster. Assign each observation to the centroid to which it is closest, in terms of Euclidean distance. Report the cluster labels for each observation. Repeat (c) and (d) until the answers obtained stop changing. In your plot from (a), color the observations according to the cluster labels obtained. 12.1.4 Question 4 Suppose that for a particular data set, we perform hierarchical clustering using single linkage and using complete linkage. We obtain two dendrograms. At a certain point on the single linkage dendrogram, the clusters {1, 2, 3} and {4, 5} fuse. On the complete linkage dendrogram, the clusters {1, 2, 3} and {4, 5} also fuse at a certain point. Which fusion will occur higher on the tree, or will they fuse at the same height, or is there not enough information to tell? At a certain point on the single linkage dendrogram, the clusters {5} and {6} fuse. On the complete linkage dendrogram, the clusters {5} and {6} also fuse at a certain point. Which fusion will occur higher on the tree, or will they fuse at the same height, or is there not enough information to tell? 12.1.5 Question 5 In words, describe the results that you would expect if you performed \\(K\\)-means clustering of the eight shoppers in Figure 12.16, on the basis of their sock and computer purchases, with \\(K = 2\\). Give three answers, one for each of the variable scalings displayed. Explain. 12.1.6 Question 6 We saw in Section 12.2.2 that the principal component loading and score vectors provide an approximation to a matrix, in the sense of (12.5). Specifically, the principal component score and loading vectors solve the optimization problem given in (12.6). Now, suppose that the M principal component score vectors zim, \\(m = 1,...,M\\), are known. Using (12.6), explain that the first \\(M\\) principal component loading vectors \\(\\phi_{jm}\\), \\(m = 1,...,M\\), can be obtaining by performing \\(M\\) separate least squares linear regressions. In each regression, the principal component score vectors are the predictors, and one of the features of the data matrix is the response. 12.2 Applied 12.2.1 Question 7 In the chapter, we mentioned the use of correlation-based distance and Euclidean distance as dissimilarity measures for hierarchical clustering. It turns out that these two measures are almost equivalent: if each observation has been centered to have mean zero and standard deviation one, and if we let \\(r_{ij}\\) denote the correlation between the \\(i\\)th and \\(j\\)th observations, then the quantity \\(1 − r_{ij}\\) is proportional to the squared Euclidean distance between the ith and jth observations. On the USArrests data, show that this proportionality holds. Hint: The Euclidean distance can be calculated using the dist() function, and correlations can be calculated using the cor() function. 12.2.2 Question 8 In Section 12.2.3, a formula for calculating PVE was given in Equation 12.10. We also saw that the PVE can be obtained using the sdev output of the prcomp() function. On the USArrests data, calculate PVE in two ways: Using the sdev output of the prcomp() function, as was done in Section 12.2.3. By applying Equation 12.10 directly. That is, use the prcomp() function to compute the principal component loadings. Then, use those loadings in Equation 12.10 to obtain the PVE. These two approaches should give the same results. Hint: You will only obtain the same results in (a) and (b) if the same data is used in both cases. For instance, if in (a) you performed prcomp() using centered and scaled variables, then you must center and scale the variables before applying Equation 12.10 in (b). 12.2.3 Question 9 Consider the USArrests data. We will now perform hierarchical clustering on the states. Using hierarchical clustering with complete linkage and Euclidean distance, cluster the states. Cut the dendrogram at a height that results in three distinct clusters. Which states belong to which clusters? Hierarchically cluster the states using complete linkage and Euclidean distance, after scaling the variables to have standard deviation one. What effect does scaling the variables have on the hierarchical clustering obtained? In your opinion, should the variables be scaled before the inter-observation dissimilarities are computed? Provide a justification for your answer. 12.2.4 Question 10 In this problem, you will generate simulated data, and then perform PCA and \\(K\\)-means clustering on the data. Generate a simulated data set with 20 observations in each of three classes (i.e. 60 observations total), and 50 variables. Hint: There are a number of functions in R that you can use to generate data. One example is the rnorm() function; runif() is another option. Be sure to add a mean shift to the observations in each class so that there are three distinct classes. Perform PCA on the 60 observations and plot the first two principal component score vectors. Use a different color to indicate the observations in each of the three classes. If the three classes appear separated in this plot, then continue on to part (c). If not, then return to part (a) and modify the simulation so that there is greater separation between the three classes. Do not continue to part (c) until the three classes show at least some separation in the first two principal component score vectors. Perform \\(K\\)-means clustering of the observations with \\(K = 3\\). How well do the clusters that you obtained in \\(K\\)-means clustering compare to the true class labels? Hint: You can use the table() function in R to compare the true class labels to the class labels obtained by clustering. Be careful how you interpret the results: \\(K\\)-means clustering will arbitrarily number the clusters, so you cannot simply check whether the true class labels and clustering labels are the same. Perform \\(K\\)-means clustering with \\(K = 2\\). Describe your results. Now perform \\(K\\)-means clustering with \\(K = 4\\), and describe your results. Now perform \\(K\\)-means clustering with \\(K = 3\\) on the first two principal component score vectors, rather than on the raw data. That is, perform \\(K\\)-means clustering on the \\(60 \\times 2\\) matrix of which the first column is the first principal component score vector, and the second column is the second principal component score vector. Comment on the results. Using the scale() function, perform \\(K\\)-means clustering with \\(K = 3\\) on the data after scaling each variable to have standard deviation one. How do these results compare to those obtained in (b)? Explain. 12.2.5 Question 11 Write an R function to perform matrix completion as in Algorithm 12.1, and as outlined in Section 12.5.2. In each iteration, the function should keep track of the relative error, as well as the iteration count. Iterations should continue until the relative error is small enough or until some maximum number of iterations is reached (set a default value for this maximum number). Furthermore, there should be an option to print out the progress in each iteration. Test your function on the Boston data. First, standardize the features to have mean zero and standard deviation one using the scale() function. Run an experiment where you randomly leave out an increasing (and nested) number of observations from 5% to 30%, in steps of 5%. Apply Algorithm 12.1 with \\(M = 1,2,...,8\\). Display the approximation error as a function of the fraction of observations that are missing, and the value of \\(M\\), averaged over 10 repetitions of the experiment. 12.2.6 Question 12 In Section 12.5.2, Algorithm 12.1 was implemented using the svd() function. However, given the connection between the svd() function and the prcomp() function highlighted in the lab, we could have instead implemented the algorithm using prcomp(). Write a function to implement Algorithm 12.1 that makes use of prcomp() rather than svd(). 12.2.7 Question 13 On the book website, www.StatLearning.com, there is a gene expression data set (Ch10Ex11.csv) that consists of 40 tissue samples with measurements on 1,000 genes. The first 20 samples are from healthy patients, while the second 20 are from a diseased group. Load in the data using read.csv(). You will need to select header = F. Apply hierarchical clustering to the samples using correlation-based distance, and plot the dendrogram. Do the genes separate the samples into the two groups? Do your results depend on the type of linkage used? Your collaborator wants to know which genes differ the most across the two groups. Suggest a way to answer this question, and apply it here. "],["multiple-testing.html", "13 Multiple Testing 13.1 Conceptual 13.2 Applied", " 13 Multiple Testing 13.1 Conceptual 13.1.1 Question 1 Suppose we test \\(m\\) null hypotheses, all of which are true. We control the Type I error for each null hypothesis at level \\(\\alpha\\). For each sub-problem, justify your answer. In total, how many Type I errors do we expect to make? Suppose that the m tests that we perform are independent. What is the family-wise error rate associated with these m tests? Hint: If two events A and B are independent, then Pr(A ∩ B) = Pr(A) Pr(B). Suppose that \\(m = 2\\), and that the p-values for the two tests are positively correlated, so that if one is small then the other will tend to be small as well, and if one is large then the other will tend to be large. How does the family-wise error rate associated with these \\(m = 2\\) tests qualitatively compare to the answer in (b) with \\(m = 2\\)? Hint: First, suppose that the two p-values are perfectly correlated. Suppose again that \\(m = 2\\), but that now the p-values for the two tests are negatively correlated, so that if one is large then the other will tend to be small. How does the family-wise error rate associated with these \\(m = 2\\) tests qualitatively compare to the answer in (b) with \\(m = 2\\)? Hint: First, suppose that whenever one p-value is less than \\(\\alpha\\), then the other will be greater than \\(\\alpha\\). In other words, we can never reject both null hypotheses. 13.1.2 Question 2 Suppose that we test \\(m\\) hypotheses, and control the Type I error for each hypothesis at level \\(\\alpha\\). Assume that all \\(m\\) p-values are independent, and that all null hypotheses are true. Let the random variable \\(A_j\\) equal 1 if the \\(j\\)th null hypothesis is rejected, and 0 otherwise. What is the distribution of \\(A_j\\)? What is the distribution of \\(\\sum_{j=1}^m A_j\\)? What is the standard deviation of the number of Type I errors that we will make? 13.1.3 Question 3 Suppose we test \\(m\\) null hypotheses, and control the Type I error for the \\(j\\)th null hypothesis at level \\(\\alpha_j\\), for \\(j=1,...,m\\). Argue that the family-wise error rate is no greater than \\(\\sum_{j=1}^m \\alpha_j\\). 13.1.4 Question 4 Suppose we test \\(m = 10\\) hypotheses, and obtain the p-values shown in Table 13.4. Suppose that we wish to control the Type I error for each null hypothesis at level \\(\\alpha = 0.05\\). Which null hypotheses will we reject? Now suppose that we wish to control the FWER at level \\(\\alpha = 0.05\\). Which null hypotheses will we reject? Justify your answer. Now suppose that we wish to control the FDR at level \\(q = 0.05\\). Which null hypotheses will we reject? Justify your answer. Now suppose that we wish to control the FDR at level \\(q = 0.2\\). Which null hypotheses will we reject? Justify your answer. Of the null hypotheses rejected at FDR level \\(q = 0.2\\), approximately how many are false positives? Justify your answer. 13.1.5 Question 5 For this problem, you will make up p-values that lead to a certain number of rejections using the Bonferroni and Holm procedures. Give an example of five p-values (i.e. five numbers between 0 and 1 which, for the purpose of this problem, we will interpret as p-values) for which both Bonferroni’s method and Holm’s method reject exactly one null hypothesis when controlling the FWER at level 0.1. Now give an example of five p-values for which Bonferroni rejects one null hypothesis and Holm rejects more than one null hypothesis at level 0.1. 13.1.6 Question 6 For each of the three panels in Figure 13.3, answer the following questions: How many false positives, false negatives, true positives, true negatives, Type I errors, and Type II errors result from applying the Bonferroni procedure to control the FWER at level \\(\\alpha = 0.05\\)? How many false positives, false negatives, true positives, true negatives, Type I errors, and Type II errors result from applying the Holm procedure to control the FWER at level \\(\\alpha = 0.05\\)? What is the false discovery rate associated with using the Bonferroni procedure to control the FWER at level \\(\\alpha = 0.05\\)? What is the false discovery rate associated with using the Holm procedure to control the FWER at level \\(\\alpha = 0.05\\)? How would the answers to (a) and (c) change if we instead used the Bonferroni procedure to control the FWER at level \\(\\alpha = 0.001\\)? 13.2 Applied 13.2.1 Question 7 This problem makes use of the Carseats dataset in the ISLR2 package. For each quantitative variable in the dataset besides Sales, fit a linear model to predict Sales using that quantitative variable. Report the p-values associated with the coefficients for the variables. That is, for each model of the form \\(Y = \\beta_0 + \\beta_1X + \\epsilon\\), report the p-value associated with the coefficient \\(\\beta_1\\). Here, \\(Y\\) represents Sales and \\(X\\) represents one of the other quantitative variables. Suppose we control the Type I error at level \\(\\alpha = 0.05\\) for the p-values obtained in (a). Which null hypotheses do we reject? Now suppose we control the FWER at level 0.05 for the p-values. Which null hypotheses do we reject? Finally, suppose we control the FDR at level 0.2 for the p-values. Which null hypotheses do we reject? 13.2.2 Question 8 In this problem, we will simulate data from \\(m = 100\\) fund managers. set.seed(1) n &lt;- 20 m &lt;- 100 X &lt;- matrix(rnorm(n * m), ncol = m) These data represent each fund manager’s percentage returns for each of \\(n = 20\\) months. We wish to test the null hypothesis that each fund manager’s percentage returns have population mean equal to zero. Notice that we simulated the data in such a way that each fund manager’s percentage returns do have population mean zero; in other words, all \\(m\\) null hypotheses are true. Conduct a one-sample \\(t\\)-test for each fund manager, and plot a histogram of the \\(p\\)-values obtained. If we control Type I error for each null hypothesis at level \\(\\alpha = 0.05\\), then how many null hypotheses do we reject? If we control the FWER at level 0.05, then how many null hypotheses do we reject? If we control the FDR at level 0.05, then how many null hypotheses do we reject? Now suppose we “cherry-pick” the 10 fund managers who perform the best in our data. If we control the FWER for just these 10 fund managers at level 0.05, then how many null hypotheses do we reject? If we control the FDR for just these 10 fund managers at level 0.05, then how many null hypotheses do we reject? Explain why the analysis in (e) is misleading. Hint The standard approaches for controlling the FWER and FDR assume that all tested null hypotheses are adjusted for multiplicity, and that no “cherry-picking” of the smallest p-values has occurred. What goes wrong if we cherry-pick? "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
